{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ogut77/DataScience/blob/main/Homework6V5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk6nkafwIHMh"
      },
      "source": [
        "# Case Intro\n",
        "Term deposits are a major source of income for a bank. A term deposit is a cash investment held at a financial institution. Your money is invested for an agreed rate of interest over a fixed amount of time, or term. The bank has various outreach plans to sell term deposits to their customers such as email marketing, advertisements, telephonic marketing, and digital marketing.\n",
        "\n",
        "Telephonic marketing campaigns still remain one of the most effective way to reach out to people. However, they require huge investment as large call centers are hired to actually execute these campaigns. Hence, it is crucial to identify the customers most likely to convert beforehand so that they can be specifically targeted via call.\n",
        "\n",
        "The data is related to direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict if the client will subscribe to a term deposit (variable y).\n",
        "\n",
        "Content\n",
        "The data is related to the direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed by the customer or not. The data folder contains two datasets:-\n",
        "\n",
        "Bank.csv: 45,211 rows and 18 columns ordered by date (from May 2008 to November 2010)\n",
        "\n",
        "Detailed Column Descriptions\n",
        "bank client data:\n",
        "\n",
        "1 - age (numeric)\n",
        "\n",
        "2 - job : type of job (categorical: \"admin.\",\"unknown\",\"unemployed\",\"management\",\"housemaid\",\"entrepreneur\",\"student\",\n",
        "\"blue-collar\",\"self-employed\",\"retired\",\"technician\",\"services\")\n",
        "\n",
        "3 - marital : marital status (categorical: \"married\",\"divorced\",\"single\"; note: \"divorced\" means divorced or widowed)\n",
        "\n",
        "4 - education (categorical: \"unknown\",\"secondary\",\"primary\",\"tertiary\")\n",
        "\n",
        "5 - default: has credit in default? (binary: \"yes\",\"no\")\n",
        "\n",
        "6 - balance: average yearly balance, in euros (numeric)\n",
        "\n",
        "7 - housing: has housing loan? (binary: \"yes\",\"no\")\n",
        "\n",
        "8 - loan: has personal loan? (binary: \"yes\",\"no\")\n",
        "# related with the last contact of the current campaign:\n",
        "9 - contact: contact communication type (categorical: \"unknown\",\"telephone\",\"cellular\")\n",
        "10 - day: last contact day of the month (numeric)\n",
        "\n",
        "11 - month: last contact month of year (categorical: \"jan\", \"feb\", \"mar\", …, \"nov\", \"dec\")\n",
        "\n",
        "12 - duration: last contact duration, in seconds (numeric)\n",
        "\n",
        "# other attributes:\n",
        "13 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
        "\n",
        "14 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted)\n",
        "\n",
        "15 - previous: number of contacts performed before this campaign and for this client (numeric)\n",
        "\n",
        "16 - poutcome: outcome of the previous marketing campaign (categorical: \"unknown\",\"other\",\"failure\",\"success\")\n",
        "\n",
        "Output variable (desired target):\n",
        "\n",
        "17 - y - has the client subscribed a term deposit? (binary: \"yes\",\"no\")\n",
        "\n",
        "Missing Attribute Values: None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "EZdL1N4MZqZF",
        "outputId": "27f7b223-54f5-4b20-f196-00757056a850"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>job</th>\n",
              "      <th>marital</th>\n",
              "      <th>education</th>\n",
              "      <th>default</th>\n",
              "      <th>balance</th>\n",
              "      <th>housing</th>\n",
              "      <th>loan</th>\n",
              "      <th>contact</th>\n",
              "      <th>day</th>\n",
              "      <th>month</th>\n",
              "      <th>duration</th>\n",
              "      <th>campaign</th>\n",
              "      <th>pdays</th>\n",
              "      <th>previous</th>\n",
              "      <th>poutcome</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>58</td>\n",
              "      <td>management</td>\n",
              "      <td>married</td>\n",
              "      <td>tertiary</td>\n",
              "      <td>no</td>\n",
              "      <td>2143</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>unknown</td>\n",
              "      <td>5</td>\n",
              "      <td>may</td>\n",
              "      <td>261</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>44</td>\n",
              "      <td>technician</td>\n",
              "      <td>single</td>\n",
              "      <td>secondary</td>\n",
              "      <td>no</td>\n",
              "      <td>29</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>unknown</td>\n",
              "      <td>5</td>\n",
              "      <td>may</td>\n",
              "      <td>151</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33</td>\n",
              "      <td>entrepreneur</td>\n",
              "      <td>married</td>\n",
              "      <td>secondary</td>\n",
              "      <td>no</td>\n",
              "      <td>2</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>unknown</td>\n",
              "      <td>5</td>\n",
              "      <td>may</td>\n",
              "      <td>76</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>47</td>\n",
              "      <td>blue-collar</td>\n",
              "      <td>married</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "      <td>1506</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>unknown</td>\n",
              "      <td>5</td>\n",
              "      <td>may</td>\n",
              "      <td>92</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33</td>\n",
              "      <td>unknown</td>\n",
              "      <td>single</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "      <td>1</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>unknown</td>\n",
              "      <td>5</td>\n",
              "      <td>may</td>\n",
              "      <td>198</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45206</th>\n",
              "      <td>51</td>\n",
              "      <td>technician</td>\n",
              "      <td>married</td>\n",
              "      <td>tertiary</td>\n",
              "      <td>no</td>\n",
              "      <td>825</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>cellular</td>\n",
              "      <td>17</td>\n",
              "      <td>nov</td>\n",
              "      <td>977</td>\n",
              "      <td>3</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>unknown</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45207</th>\n",
              "      <td>71</td>\n",
              "      <td>retired</td>\n",
              "      <td>divorced</td>\n",
              "      <td>primary</td>\n",
              "      <td>no</td>\n",
              "      <td>1729</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>cellular</td>\n",
              "      <td>17</td>\n",
              "      <td>nov</td>\n",
              "      <td>456</td>\n",
              "      <td>2</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>unknown</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45208</th>\n",
              "      <td>72</td>\n",
              "      <td>retired</td>\n",
              "      <td>married</td>\n",
              "      <td>secondary</td>\n",
              "      <td>no</td>\n",
              "      <td>5715</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>cellular</td>\n",
              "      <td>17</td>\n",
              "      <td>nov</td>\n",
              "      <td>1127</td>\n",
              "      <td>5</td>\n",
              "      <td>184</td>\n",
              "      <td>3</td>\n",
              "      <td>success</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45209</th>\n",
              "      <td>57</td>\n",
              "      <td>blue-collar</td>\n",
              "      <td>married</td>\n",
              "      <td>secondary</td>\n",
              "      <td>no</td>\n",
              "      <td>668</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>telephone</td>\n",
              "      <td>17</td>\n",
              "      <td>nov</td>\n",
              "      <td>508</td>\n",
              "      <td>4</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45210</th>\n",
              "      <td>37</td>\n",
              "      <td>entrepreneur</td>\n",
              "      <td>married</td>\n",
              "      <td>secondary</td>\n",
              "      <td>no</td>\n",
              "      <td>2971</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>cellular</td>\n",
              "      <td>17</td>\n",
              "      <td>nov</td>\n",
              "      <td>361</td>\n",
              "      <td>2</td>\n",
              "      <td>188</td>\n",
              "      <td>11</td>\n",
              "      <td>other</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>45211 rows × 17 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       age           job   marital  education default  balance housing loan  \\\n",
              "0       58    management   married   tertiary      no     2143     yes   no   \n",
              "1       44    technician    single  secondary      no       29     yes   no   \n",
              "2       33  entrepreneur   married  secondary      no        2     yes  yes   \n",
              "3       47   blue-collar   married    unknown      no     1506     yes   no   \n",
              "4       33       unknown    single    unknown      no        1      no   no   \n",
              "...    ...           ...       ...        ...     ...      ...     ...  ...   \n",
              "45206   51    technician   married   tertiary      no      825      no   no   \n",
              "45207   71       retired  divorced    primary      no     1729      no   no   \n",
              "45208   72       retired   married  secondary      no     5715      no   no   \n",
              "45209   57   blue-collar   married  secondary      no      668      no   no   \n",
              "45210   37  entrepreneur   married  secondary      no     2971      no   no   \n",
              "\n",
              "         contact  day month  duration  campaign  pdays  previous poutcome    y  \n",
              "0        unknown    5   may       261         1     -1         0  unknown   no  \n",
              "1        unknown    5   may       151         1     -1         0  unknown   no  \n",
              "2        unknown    5   may        76         1     -1         0  unknown   no  \n",
              "3        unknown    5   may        92         1     -1         0  unknown   no  \n",
              "4        unknown    5   may       198         1     -1         0  unknown   no  \n",
              "...          ...  ...   ...       ...       ...    ...       ...      ...  ...  \n",
              "45206   cellular   17   nov       977         3     -1         0  unknown  yes  \n",
              "45207   cellular   17   nov       456         2     -1         0  unknown  yes  \n",
              "45208   cellular   17   nov      1127         5    184         3  success  yes  \n",
              "45209  telephone   17   nov       508         4     -1         0  unknown   no  \n",
              "45210   cellular   17   nov       361         2    188        11    other   no  \n",
              "\n",
              "[45211 rows x 17 columns]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "df=pd.read_csv('https://raw.githubusercontent.com/ogut77/DataScience/main/data/Bank.csv',sep = ';')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "import optuna\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hm9ZMVW9aJNh",
        "outputId": "5a10d7c4-fd61-4043-b03c-33d9cb88ba95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(45211, 17)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 45211 entries, 0 to 45210\n",
            "Data columns (total 17 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   age        45211 non-null  int64 \n",
            " 1   job        45211 non-null  object\n",
            " 2   marital    45211 non-null  object\n",
            " 3   education  45211 non-null  object\n",
            " 4   default    45211 non-null  object\n",
            " 5   balance    45211 non-null  int64 \n",
            " 6   housing    45211 non-null  object\n",
            " 7   loan       45211 non-null  object\n",
            " 8   contact    45211 non-null  object\n",
            " 9   day        45211 non-null  int64 \n",
            " 10  month      45211 non-null  object\n",
            " 11  duration   45211 non-null  int64 \n",
            " 12  campaign   45211 non-null  int64 \n",
            " 13  pdays      45211 non-null  int64 \n",
            " 14  previous   45211 non-null  int64 \n",
            " 15  poutcome   45211 non-null  object\n",
            " 16  y          45211 non-null  object\n",
            "dtypes: int64(7), object(10)\n",
            "memory usage: 5.9+ MB\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "age          0\n",
              "job          0\n",
              "marital      0\n",
              "education    0\n",
              "default      0\n",
              "balance      0\n",
              "housing      0\n",
              "loan         0\n",
              "contact      0\n",
              "day          0\n",
              "month        0\n",
              "duration     0\n",
              "campaign     0\n",
              "pdays        0\n",
              "previous     0\n",
              "poutcome     0\n",
              "y            0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(df.shape)\n",
        "df.info()\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LD1CE208fhi",
        "outputId": "914d70dc-8ab0-4faa-abd5-ca4b28fb22cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "job\n",
            "blue-collar      9732\n",
            "management       9458\n",
            "technician       7597\n",
            "admin.           5171\n",
            "services         4154\n",
            "retired          2264\n",
            "self-employed    1579\n",
            "entrepreneur     1487\n",
            "unemployed       1303\n",
            "housemaid        1240\n",
            "student           938\n",
            "unknown           288\n",
            "Name: count, dtype: int64\n",
            "marital\n",
            "married     27214\n",
            "single      12790\n",
            "divorced     5207\n",
            "Name: count, dtype: int64\n",
            "education\n",
            "secondary    23202\n",
            "tertiary     13301\n",
            "primary       6851\n",
            "unknown       1857\n",
            "Name: count, dtype: int64\n",
            "default\n",
            "no     44396\n",
            "yes      815\n",
            "Name: count, dtype: int64\n",
            "housing\n",
            "yes    25130\n",
            "no     20081\n",
            "Name: count, dtype: int64\n",
            "loan\n",
            "no     37967\n",
            "yes     7244\n",
            "Name: count, dtype: int64\n",
            "contact\n",
            "cellular     29285\n",
            "unknown      13020\n",
            "telephone     2906\n",
            "Name: count, dtype: int64\n",
            "month\n",
            "may    13766\n",
            "jul     6895\n",
            "aug     6247\n",
            "jun     5341\n",
            "nov     3970\n",
            "apr     2932\n",
            "feb     2649\n",
            "jan     1403\n",
            "oct      738\n",
            "sep      579\n",
            "mar      477\n",
            "dec      214\n",
            "Name: count, dtype: int64\n",
            "poutcome\n",
            "unknown    36959\n",
            "failure     4901\n",
            "other       1840\n",
            "success     1511\n",
            "Name: count, dtype: int64\n",
            "y\n",
            "no     39922\n",
            "yes     5289\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#For object check the data\n",
        "for cn in df.columns:\n",
        "  if(df[cn].dtype==object):\n",
        "    print(df[cn].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "JqEV5gKmD6_v"
      },
      "outputs": [],
      "source": [
        "# Following function converts non-numeric variables (e.g., 'category', 'object') into numeric using label encoding\n",
        "# Note:\n",
        "#Label encoding converts categorical values into integer codes. Each unique category is assigned a distinct number, such as 1, 2, 3, 4, etc.\n",
        "#Example : In our data, education variable has following values:{ 'primary' 'secondary', 'tertiary', 'unknown'}\n",
        "#and it is mapped { 'primary': 0, 'secondary': 1, 'tertiary': 2, 'unknown': 3}\n",
        "\n",
        "#  education  education_encoded\n",
        "#  secondary                  1\n",
        "#   tertiary                  2\n",
        "#    primary                  0\n",
        "#    unknown                  3\n",
        "#   tertiary                  2\n",
        "#    primary                  0\n",
        "#\n",
        "#Label encoding is generally suitable for tree-based models (e.g., decision trees, random forests, boosting methods).\n",
        "#However, it may not be appropriate for models where the objective function relies on distance-based calculations, such as neural networks, support vector machines (SVM), or linear regression.\n",
        "#For nominal features (categories with no intrinsic ordering, e.g., \"red,\" \"blue,\" \"green\"), label encoding can mislead the model by implying an ordinal relationship where none exists.\n",
        "#In such cases, one-hot encoding is usually preferred.\n",
        "#One-hot encoding creates binary columns (dummy variables) for each category in a categorical feature, avoiding the introduction of unintended ordinality.\n",
        "\n",
        "def Encoder(df):\n",
        "          from sklearn import preprocessing\n",
        "          columnsToEncode = list(df.select_dtypes(include=['category','object']))\n",
        "          le = preprocessing.LabelEncoder()\n",
        "          for feature in columnsToEncode:\n",
        "              try:\n",
        "                  df[feature] = le.fit_transform(df[feature])\n",
        "              except:\n",
        "                  print('Error encoding '+feature)\n",
        "          return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "U_XhnTUFBfMj",
        "outputId": "564863b0-b070-46d6-c051-71be0e5db42c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>job</th>\n",
              "      <th>marital</th>\n",
              "      <th>education</th>\n",
              "      <th>default</th>\n",
              "      <th>balance</th>\n",
              "      <th>housing</th>\n",
              "      <th>loan</th>\n",
              "      <th>contact</th>\n",
              "      <th>day</th>\n",
              "      <th>month</th>\n",
              "      <th>duration</th>\n",
              "      <th>campaign</th>\n",
              "      <th>pdays</th>\n",
              "      <th>previous</th>\n",
              "      <th>poutcome</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>58</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2143</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>261</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>44</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>151</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>76</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>47</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1506</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>92</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>198</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45206</th>\n",
              "      <td>51</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>825</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>977</td>\n",
              "      <td>3</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45207</th>\n",
              "      <td>71</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1729</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>456</td>\n",
              "      <td>2</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45208</th>\n",
              "      <td>72</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5715</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>1127</td>\n",
              "      <td>5</td>\n",
              "      <td>184</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45209</th>\n",
              "      <td>57</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>668</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>508</td>\n",
              "      <td>4</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45210</th>\n",
              "      <td>37</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2971</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>361</td>\n",
              "      <td>2</td>\n",
              "      <td>188</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>45211 rows × 17 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       age  job  marital  education  default  balance  housing  loan  contact  \\\n",
              "0       58    4        1          2        0     2143        1     0        2   \n",
              "1       44    9        2          1        0       29        1     0        2   \n",
              "2       33    2        1          1        0        2        1     1        2   \n",
              "3       47    1        1          3        0     1506        1     0        2   \n",
              "4       33   11        2          3        0        1        0     0        2   \n",
              "...    ...  ...      ...        ...      ...      ...      ...   ...      ...   \n",
              "45206   51    9        1          2        0      825        0     0        0   \n",
              "45207   71    5        0          0        0     1729        0     0        0   \n",
              "45208   72    5        1          1        0     5715        0     0        0   \n",
              "45209   57    1        1          1        0      668        0     0        1   \n",
              "45210   37    2        1          1        0     2971        0     0        0   \n",
              "\n",
              "       day  month  duration  campaign  pdays  previous  poutcome  y  \n",
              "0        5      8       261         1     -1         0         3  0  \n",
              "1        5      8       151         1     -1         0         3  0  \n",
              "2        5      8        76         1     -1         0         3  0  \n",
              "3        5      8        92         1     -1         0         3  0  \n",
              "4        5      8       198         1     -1         0         3  0  \n",
              "...    ...    ...       ...       ...    ...       ...       ... ..  \n",
              "45206   17      9       977         3     -1         0         3  1  \n",
              "45207   17      9       456         2     -1         0         3  1  \n",
              "45208   17      9      1127         5    184         3         2  1  \n",
              "45209   17      9       508         4     -1         0         3  0  \n",
              "45210   17      9       361         2    188        11         1  0  \n",
              "\n",
              "[45211 rows x 17 columns]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df=Encoder(df)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "s8C7J0ObyNs5",
        "outputId": "9eb388cf-5356-429d-f571-43d9f37e7526"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>job</th>\n",
              "      <th>marital</th>\n",
              "      <th>education</th>\n",
              "      <th>default</th>\n",
              "      <th>balance</th>\n",
              "      <th>housing</th>\n",
              "      <th>loan</th>\n",
              "      <th>contact</th>\n",
              "      <th>day</th>\n",
              "      <th>month</th>\n",
              "      <th>duration</th>\n",
              "      <th>campaign</th>\n",
              "      <th>pdays</th>\n",
              "      <th>previous</th>\n",
              "      <th>poutcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>58</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2143</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>261</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>44</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>151</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>76</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>47</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1506</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>92</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>198</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45206</th>\n",
              "      <td>51</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>825</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>977</td>\n",
              "      <td>3</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45207</th>\n",
              "      <td>71</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1729</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>456</td>\n",
              "      <td>2</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45208</th>\n",
              "      <td>72</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5715</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>1127</td>\n",
              "      <td>5</td>\n",
              "      <td>184</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45209</th>\n",
              "      <td>57</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>668</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>508</td>\n",
              "      <td>4</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45210</th>\n",
              "      <td>37</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2971</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>361</td>\n",
              "      <td>2</td>\n",
              "      <td>188</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>45211 rows × 16 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       age  job  marital  education  default  balance  housing  loan  contact  \\\n",
              "0       58    4        1          2        0     2143        1     0        2   \n",
              "1       44    9        2          1        0       29        1     0        2   \n",
              "2       33    2        1          1        0        2        1     1        2   \n",
              "3       47    1        1          3        0     1506        1     0        2   \n",
              "4       33   11        2          3        0        1        0     0        2   \n",
              "...    ...  ...      ...        ...      ...      ...      ...   ...      ...   \n",
              "45206   51    9        1          2        0      825        0     0        0   \n",
              "45207   71    5        0          0        0     1729        0     0        0   \n",
              "45208   72    5        1          1        0     5715        0     0        0   \n",
              "45209   57    1        1          1        0      668        0     0        1   \n",
              "45210   37    2        1          1        0     2971        0     0        0   \n",
              "\n",
              "       day  month  duration  campaign  pdays  previous  poutcome  \n",
              "0        5      8       261         1     -1         0         3  \n",
              "1        5      8       151         1     -1         0         3  \n",
              "2        5      8        76         1     -1         0         3  \n",
              "3        5      8        92         1     -1         0         3  \n",
              "4        5      8       198         1     -1         0         3  \n",
              "...    ...    ...       ...       ...    ...       ...       ...  \n",
              "45206   17      9       977         3     -1         0         3  \n",
              "45207   17      9       456         2     -1         0         3  \n",
              "45208   17      9      1127         5    184         3         2  \n",
              "45209   17      9       508         4     -1         0         3  \n",
              "45210   17      9       361         2    188        11         1  \n",
              "\n",
              "[45211 rows x 16 columns]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = df['y'] #Output\n",
        "X = df.drop('y',axis=1)\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "aC4sGAvybIGe"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import  train_test_split\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=17)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbZLvi_k2kjt"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Q1)Using  Random Forest,XGBoost, Light GBM and Gradient Boosting Classifier with default parameters (no parameter specifications except random_state) calculate Accuracy on Test data. Which method gives the best accuracy on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 3940, number of negative: 29968\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001294 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 988\n",
            "[LightGBM] [Info] Number of data points in the train set: 33908, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116197 -> initscore=-2.028949\n",
            "[LightGBM] [Info] Start training from score -2.028949\n",
            "Random Forest: Accuracy = 0.9025\n",
            "Gradient Boosting: Accuracy = 0.9019\n",
            "XGBoost: Accuracy = 0.9037\n",
            "LightGBM: Accuracy = 0.9074\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Initialize classifiers with random_state\n",
        "models = {\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=17),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=17),\n",
        "    \"XGBoost\": XGBClassifier(random_state=17),\n",
        "    \"LightGBM\": LGBMClassifier(random_state=17)\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "accuracy_results = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy_results[name] = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Display accuracy for each model\n",
        "for model_name, acc in accuracy_results.items():\n",
        "    print(f\"{model_name}: Accuracy = {acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LightGBM is better \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK77k75qO7dW"
      },
      "source": [
        "Q2) Using optuna hyperparmeter optimization technique and 100 trial\n",
        "\n",
        " a)find best methods with  parameters  using Cross validation (CV=3) technique for the range of   parameters below. What are the best parameters for the method with highest cross validation accuracy?\n",
        " For random forest\n",
        "\n",
        "\n",
        "  \"max_depth\"   : trial.suggest_int(\"max_depth\", 2,  X_train.shape[1]),\n",
        "  \"max_features\": trial.suggest_int(\"max_features\", 2, X_train.shape[1])\n",
        "\n",
        "For XGBoost, Light GBM and Gradient Boosting Classifier\n",
        "\n",
        "  \"max_depth\": trial.suggest_int(\"max_depth\", 2, X_train.shape[1]),\n",
        "  \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.3,log=True)\n",
        "\n",
        "where X_train.shape[1] is number of columnns in the train data.\n",
        "\n",
        " b)Evaluate the performance of the  method with highest cross validation accuracy on test data.What is the accuracy value? Are there any improvement of the same method with default parameters?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:39:48,926] A new study created in memory with name: RandomForest\n",
            "[I 2025-04-15 15:39:59,313] Trial 0 finished with value: 0.9035626435580686 and parameters: {'max_depth': 7, 'max_features': 13}. Best is trial 0 with value: 0.9035626435580686.\n",
            "[I 2025-04-15 15:40:07,370] Trial 1 finished with value: 0.9009968538787275 and parameters: {'max_depth': 5, 'max_features': 16}. Best is trial 0 with value: 0.9035626435580686.\n",
            "[I 2025-04-15 15:40:20,585] Trial 2 finished with value: 0.9039460173833117 and parameters: {'max_depth': 15, 'max_features': 10}. Best is trial 2 with value: 0.9039460173833117.\n",
            "[I 2025-04-15 15:40:28,483] Trial 3 finished with value: 0.9035921446910912 and parameters: {'max_depth': 7, 'max_features': 10}. Best is trial 2 with value: 0.9039460173833117.\n",
            "[I 2025-04-15 15:40:45,482] Trial 4 finished with value: 0.9037100683338194 and parameters: {'max_depth': 9, 'max_features': 15}. Best is trial 2 with value: 0.9039460173833117.\n",
            "[I 2025-04-15 15:40:50,723] Trial 5 finished with value: 0.9030022551066876 and parameters: {'max_depth': 9, 'max_features': 4}. Best is trial 2 with value: 0.9039460173833117.\n",
            "[I 2025-04-15 15:40:57,451] Trial 6 finished with value: 0.9030612965127465 and parameters: {'max_depth': 6, 'max_features': 10}. Best is trial 2 with value: 0.9039460173833117.\n",
            "[I 2025-04-15 15:41:02,041] Trial 7 finished with value: 0.9004365071766948 and parameters: {'max_depth': 13, 'max_features': 2}. Best is trial 2 with value: 0.9039460173833117.\n",
            "[I 2025-04-15 15:41:06,152] Trial 8 finished with value: 0.899817194739296 and parameters: {'max_depth': 7, 'max_features': 4}. Best is trial 2 with value: 0.9039460173833117.\n",
            "[I 2025-04-15 15:41:14,210] Trial 9 finished with value: 0.9051846631327827 and parameters: {'max_depth': 9, 'max_features': 7}. Best is trial 9 with value: 0.9051846631327827.\n",
            "[I 2025-04-15 15:41:16,441] Trial 10 finished with value: 0.883921200944627 and parameters: {'max_depth': 2, 'max_features': 7}. Best is trial 9 with value: 0.9051846631327827.\n",
            "[I 2025-04-15 15:41:28,613] Trial 11 finished with value: 0.9048307747845565 and parameters: {'max_depth': 16, 'max_features': 8}. Best is trial 9 with value: 0.9051846631327827.\n",
            "[I 2025-04-15 15:41:37,858] Trial 12 finished with value: 0.9047128224391514 and parameters: {'max_depth': 12, 'max_features': 7}. Best is trial 9 with value: 0.9051846631327827.\n",
            "[I 2025-04-15 15:41:48,086] Trial 13 finished with value: 0.9038280206792244 and parameters: {'max_depth': 16, 'max_features': 7}. Best is trial 9 with value: 0.9051846631327827.\n",
            "[I 2025-04-15 15:42:03,731] Trial 14 finished with value: 0.9045063640853447 and parameters: {'max_depth': 13, 'max_features': 12}. Best is trial 9 with value: 0.9051846631327827.\n",
            "[I 2025-04-15 15:42:10,505] Trial 15 finished with value: 0.9046243373054237 and parameters: {'max_depth': 11, 'max_features': 5}. Best is trial 9 with value: 0.9051846631327827.\n",
            "[I 2025-04-15 15:42:19,117] Trial 16 finished with value: 0.9046832952127866 and parameters: {'max_depth': 10, 'max_features': 8}. Best is trial 9 with value: 0.9051846631327827.\n",
            "[I 2025-04-15 15:42:21,148] Trial 17 finished with value: 0.8866934098377611 and parameters: {'max_depth': 3, 'max_features': 5}. Best is trial 9 with value: 0.9051846631327827.\n",
            "[I 2025-04-15 15:42:35,512] Trial 18 finished with value: 0.9040934786897421 and parameters: {'max_depth': 14, 'max_features': 12}. Best is trial 9 with value: 0.9051846631327827.\n",
            "[I 2025-04-15 15:42:46,806] Trial 19 finished with value: 0.9031497242411207 and parameters: {'max_depth': 16, 'max_features': 9}. Best is trial 9 with value: 0.9051846631327827.\n",
            "[I 2025-04-15 15:42:50,224] Trial 20 finished with value: 0.8979591817564074 and parameters: {'max_depth': 11, 'max_features': 2}. Best is trial 9 with value: 0.9051846631327827.\n",
            "[I 2025-04-15 15:42:58,324] Trial 21 finished with value: 0.9047128224391514 and parameters: {'max_depth': 12, 'max_features': 7}. Best is trial 9 with value: 0.9051846631327827.\n",
            "[I 2025-04-15 15:43:06,566] Trial 22 finished with value: 0.9050667368807201 and parameters: {'max_depth': 14, 'max_features': 6}. Best is trial 9 with value: 0.9051846631327827.\n",
            "[I 2025-04-15 15:43:14,474] Trial 23 finished with value: 0.90441786851428 and parameters: {'max_depth': 14, 'max_features': 5}. Best is trial 9 with value: 0.9051846631327827.\n",
            "[I 2025-04-15 15:43:25,192] Trial 24 finished with value: 0.9048307747845565 and parameters: {'max_depth': 16, 'max_features': 8}. Best is trial 9 with value: 0.9051846631327827.\n",
            "[I 2025-04-15 15:43:33,336] Trial 25 finished with value: 0.9050667368807201 and parameters: {'max_depth': 14, 'max_features': 6}. Best is trial 9 with value: 0.9051846631327827.\n",
            "[I 2025-04-15 15:43:41,890] Trial 26 finished with value: 0.9050667368807201 and parameters: {'max_depth': 14, 'max_features': 6}. Best is trial 9 with value: 0.9051846631327827.\n",
            "[I 2025-04-15 15:43:45,868] Trial 27 finished with value: 0.9001710883061907 and parameters: {'max_depth': 10, 'max_features': 3}. Best is trial 9 with value: 0.9051846631327827.\n",
            "[I 2025-04-15 15:43:51,785] Trial 28 finished with value: 0.9045063823506845 and parameters: {'max_depth': 8, 'max_features': 6}. Best is trial 9 with value: 0.9051846631327827.\n",
            "[I 2025-04-15 15:43:54,318] Trial 29 finished with value: 0.8934175154661373 and parameters: {'max_depth': 5, 'max_features': 4}. Best is trial 9 with value: 0.9051846631327827.\n",
            "[I 2025-04-15 15:44:01,521] Trial 30 finished with value: 0.904889750957259 and parameters: {'max_depth': 12, 'max_features': 6}. Best is trial 9 with value: 0.9051846631327827.\n",
            "[I 2025-04-15 15:44:10,490] Trial 31 finished with value: 0.9050667368807201 and parameters: {'max_depth': 14, 'max_features': 6}. Best is trial 9 with value: 0.9051846631327827.\n",
            "[I 2025-04-15 15:44:25,221] Trial 32 finished with value: 0.9039755132976657 and parameters: {'max_depth': 15, 'max_features': 9}. Best is trial 9 with value: 0.9051846631327827.\n",
            "[I 2025-04-15 15:44:39,027] Trial 33 finished with value: 0.905391105830584 and parameters: {'max_depth': 13, 'max_features': 6}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:44:46,547] Trial 34 finished with value: 0.9050961962643947 and parameters: {'max_depth': 13, 'max_features': 5}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:44:50,628] Trial 35 finished with value: 0.9023534754576118 and parameters: {'max_depth': 11, 'max_features': 3}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:44:56,433] Trial 36 finished with value: 0.9043588766855719 and parameters: {'max_depth': 10, 'max_features': 5}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:45:03,717] Trial 37 finished with value: 0.9042999579182228 and parameters: {'max_depth': 8, 'max_features': 9}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:45:16,076] Trial 38 finished with value: 0.9042704333011921 and parameters: {'max_depth': 13, 'max_features': 11}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:45:21,030] Trial 39 finished with value: 0.9036806011221422 and parameters: {'max_depth': 15, 'max_features': 3}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:45:31,714] Trial 40 finished with value: 0.9036510634584402 and parameters: {'max_depth': 8, 'max_features': 15}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:45:37,231] Trial 41 finished with value: 0.904653762767753 and parameters: {'max_depth': 13, 'max_features': 4}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:45:45,390] Trial 42 finished with value: 0.9050667238340488 and parameters: {'max_depth': 15, 'max_features': 6}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:45:54,309] Trial 43 finished with value: 0.9051551646090944 and parameters: {'max_depth': 12, 'max_features': 8}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:46:03,368] Trial 44 finished with value: 0.9051551646090944 and parameters: {'max_depth': 12, 'max_features': 8}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:46:13,051] Trial 45 finished with value: 0.9051551646090944 and parameters: {'max_depth': 12, 'max_features': 8}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:46:21,789] Trial 46 finished with value: 0.9040049752906745 and parameters: {'max_depth': 9, 'max_features': 10}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:46:30,684] Trial 47 finished with value: 0.9051551646090944 and parameters: {'max_depth': 12, 'max_features': 8}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:46:40,472] Trial 48 finished with value: 0.9050667447087228 and parameters: {'max_depth': 11, 'max_features': 10}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:46:47,220] Trial 49 finished with value: 0.9051551698277628 and parameters: {'max_depth': 9, 'max_features': 8}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:46:53,309] Trial 50 finished with value: 0.9051846631327827 and parameters: {'max_depth': 9, 'max_features': 7}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:46:59,587] Trial 51 finished with value: 0.9051846631327827 and parameters: {'max_depth': 9, 'max_features': 7}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:47:05,670] Trial 52 finished with value: 0.9051846631327827 and parameters: {'max_depth': 9, 'max_features': 7}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:47:10,544] Trial 53 finished with value: 0.9037985534675471 and parameters: {'max_depth': 7, 'max_features': 7}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:47:16,642] Trial 54 finished with value: 0.9051846631327827 and parameters: {'max_depth': 9, 'max_features': 7}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:47:22,669] Trial 55 finished with value: 0.9051846631327827 and parameters: {'max_depth': 9, 'max_features': 7}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:47:26,966] Trial 56 finished with value: 0.9030022759813616 and parameters: {'max_depth': 6, 'max_features': 7}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:47:32,590] Trial 57 finished with value: 0.9045358443436932 and parameters: {'max_depth': 8, 'max_features': 7}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:47:41,059] Trial 58 finished with value: 0.9042703785051729 and parameters: {'max_depth': 10, 'max_features': 9}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:47:45,048] Trial 59 finished with value: 0.902205988057839 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:47:52,093] Trial 60 finished with value: 0.904476891654999 and parameters: {'max_depth': 10, 'max_features': 7}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:47:59,171] Trial 61 finished with value: 0.9051846631327827 and parameters: {'max_depth': 9, 'max_features': 7}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:48:18,108] Trial 62 finished with value: 0.9045948596564095 and parameters: {'max_depth': 9, 'max_features': 6}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:48:39,353] Trial 63 finished with value: 0.9051846631327827 and parameters: {'max_depth': 9, 'max_features': 7}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:49:02,573] Trial 64 finished with value: 0.9042999579182228 and parameters: {'max_depth': 8, 'max_features': 9}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:49:17,460] Trial 65 finished with value: 0.9041819690421384 and parameters: {'max_depth': 10, 'max_features': 4}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:49:30,364] Trial 66 finished with value: 0.9030022707626931 and parameters: {'max_depth': 6, 'max_features': 6}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:49:51,412] Trial 67 finished with value: 0.9044473957406449 and parameters: {'max_depth': 8, 'max_features': 8}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:50:10,587] Trial 68 finished with value: 0.9046243373054237 and parameters: {'max_depth': 11, 'max_features': 5}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:50:27,260] Trial 69 finished with value: 0.9037985534675471 and parameters: {'max_depth': 7, 'max_features': 7}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:50:44,977] Trial 70 finished with value: 0.9045948596564095 and parameters: {'max_depth': 9, 'max_features': 6}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:51:05,965] Trial 71 finished with value: 0.9051846631327827 and parameters: {'max_depth': 9, 'max_features': 7}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:51:19,050] Trial 72 finished with value: 0.904476891654999 and parameters: {'max_depth': 10, 'max_features': 7}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:51:25,772] Trial 73 finished with value: 0.9045358443436932 and parameters: {'max_depth': 8, 'max_features': 7}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:51:33,435] Trial 74 finished with value: 0.9051551698277628 and parameters: {'max_depth': 9, 'max_features': 8}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:51:44,007] Trial 75 finished with value: 0.9042703785051729 and parameters: {'max_depth': 10, 'max_features': 9}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:51:50,046] Trial 76 finished with value: 0.904358887122909 and parameters: {'max_depth': 9, 'max_features': 5}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:51:57,286] Trial 77 finished with value: 0.9050961727803863 and parameters: {'max_depth': 11, 'max_features': 6}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:52:03,021] Trial 78 finished with value: 0.9045063823506845 and parameters: {'max_depth': 8, 'max_features': 6}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:52:11,055] Trial 79 finished with value: 0.9046832952127866 and parameters: {'max_depth': 10, 'max_features': 8}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:52:17,791] Trial 80 finished with value: 0.9045358443436932 and parameters: {'max_depth': 8, 'max_features': 7}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:52:24,467] Trial 81 finished with value: 0.9051846631327827 and parameters: {'max_depth': 9, 'max_features': 7}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:52:32,301] Trial 82 finished with value: 0.9051846631327827 and parameters: {'max_depth': 9, 'max_features': 7}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:52:37,349] Trial 83 finished with value: 0.90338568372795 and parameters: {'max_depth': 7, 'max_features': 6}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:52:52,042] Trial 84 finished with value: 0.9037690627718616 and parameters: {'max_depth': 10, 'max_features': 16}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:52:54,858] Trial 85 finished with value: 0.8881679733247133 and parameters: {'max_depth': 3, 'max_features': 8}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:53:02,771] Trial 86 finished with value: 0.9053616125255642 and parameters: {'max_depth': 9, 'max_features': 9}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:53:09,955] Trial 87 finished with value: 0.9042999579182228 and parameters: {'max_depth': 8, 'max_features': 9}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:53:19,885] Trial 88 finished with value: 0.9043293990365576 and parameters: {'max_depth': 10, 'max_features': 10}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:53:34,065] Trial 89 finished with value: 0.9051551698277628 and parameters: {'max_depth': 9, 'max_features': 8}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:53:57,464] Trial 90 finished with value: 0.9049782230443156 and parameters: {'max_depth': 11, 'max_features': 11}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:54:10,036] Trial 91 finished with value: 0.9042704045985154 and parameters: {'max_depth': 9, 'max_features': 13}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:54:25,777] Trial 92 finished with value: 0.9051846631327827 and parameters: {'max_depth': 9, 'max_features': 7}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:54:39,043] Trial 93 finished with value: 0.90338568372795 and parameters: {'max_depth': 7, 'max_features': 6}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:54:55,442] Trial 94 finished with value: 0.9045358443436932 and parameters: {'max_depth': 8, 'max_features': 7}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:55:10,396] Trial 95 finished with value: 0.904358887122909 and parameters: {'max_depth': 9, 'max_features': 5}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:55:21,097] Trial 96 finished with value: 0.9046832952127866 and parameters: {'max_depth': 10, 'max_features': 8}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:55:28,562] Trial 97 finished with value: 0.9049487036459533 and parameters: {'max_depth': 10, 'max_features': 6}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:55:38,855] Trial 98 finished with value: 0.9052731717505188 and parameters: {'max_depth': 11, 'max_features': 7}. Best is trial 33 with value: 0.905391105830584.\n",
            "[I 2025-04-15 15:56:04,330] Trial 99 finished with value: 0.9044473722566367 and parameters: {'max_depth': 11, 'max_features': 8}. Best is trial 33 with value: 0.905391105830584.\n"
          ]
        }
      ],
      "source": [
        "#a\n",
        "n_features = X_train.shape[1]\n",
        "\n",
        "def rf_objective(trial):\n",
        "    params = {\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, n_features),\n",
        "        \"max_features\": trial.suggest_int(\"max_features\", 2, n_features),\n",
        "        \"n_estimators\": 100,\n",
        "        \"random_state\": 42\n",
        "    }\n",
        "    model = RandomForestClassifier(**params)\n",
        "    return cross_val_score(model, X_train, y_train, cv=3, scoring=\"accuracy\").mean()\n",
        "\n",
        "def xgb_objective(trial):\n",
        "    params = {\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, n_features),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.3, log=True),\n",
        "        \"n_estimators\": 100,\n",
        "        \"use_label_encoder\": False,\n",
        "        \"eval_metric\": \"logloss\",\n",
        "        \"random_state\": 42\n",
        "    }\n",
        "    model = XGBClassifier(**params)\n",
        "    return cross_val_score(model, X_train, y_train, cv=3, scoring=\"accuracy\").mean()\n",
        "\n",
        "def lgbm_objective(trial):\n",
        "    params = {\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, n_features),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.3, log=True),\n",
        "        \"n_estimators\": 100,\n",
        "        \"random_state\": 42\n",
        "    }\n",
        "    model = LGBMClassifier(**params)\n",
        "    return cross_val_score(model, X_train, y_train, cv=3, scoring=\"accuracy\").mean()\n",
        "\n",
        "def gb_objective(trial):\n",
        "    params = {\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, n_features),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.3, log=True),\n",
        "        \"n_estimators\": 100,\n",
        "        \"random_state\": 42\n",
        "    }\n",
        "    model = GradientBoostingClassifier(**params)\n",
        "    return cross_val_score(model, X_train, y_train, cv=3, scoring=\"accuracy\").mean()\n",
        "\n",
        "# Run studies\n",
        "def run_study(objective_func, name):\n",
        "    study = optuna.create_study(direction=\"maximize\", study_name=name)\n",
        "    study.optimize(objective_func, n_trials=100)\n",
        "    return study\n",
        "\n",
        "rf_study = run_study(rf_objective, \"RandomForest\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:56:04,360] A new study created in memory with name: XGBoost\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:08,157] Trial 0 finished with value: 0.903621619730771 and parameters: {'max_depth': 13, 'learning_rate': 0.06222207746278341}. Best is trial 0 with value: 0.903621619730771.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:08] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:08] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:09,934] Trial 1 finished with value: 0.8838032329432165 and parameters: {'max_depth': 8, 'learning_rate': 0.0052109796200883966}. Best is trial 0 with value: 0.903621619730771.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:11] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:12] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:13,575] Trial 2 finished with value: 0.9028843262452909 and parameters: {'max_depth': 13, 'learning_rate': 0.045481914108571074}. Best is trial 0 with value: 0.903621619730771.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:14,240] Trial 3 finished with value: 0.8849828894733136 and parameters: {'max_depth': 2, 'learning_rate': 0.021272617545424883}. Best is trial 0 with value: 0.903621619730771.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:16] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:18] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:20,519] Trial 4 finished with value: 0.8968680181880408 and parameters: {'max_depth': 16, 'learning_rate': 0.011987437781588365}. Best is trial 0 with value: 0.903621619730771.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:20] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:22,670] Trial 5 finished with value: 0.8838032329432165 and parameters: {'max_depth': 9, 'learning_rate': 0.0015066247180715417}. Best is trial 0 with value: 0.903621619730771.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:22] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:23] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:23] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:24,357] Trial 6 finished with value: 0.9040640349620729 and parameters: {'max_depth': 9, 'learning_rate': 0.19684570526867834}. Best is trial 6 with value: 0.9040640349620729.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:24] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:24] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:25,284] Trial 7 finished with value: 0.8966026515042219 and parameters: {'max_depth': 5, 'learning_rate': 0.01570731644489474}. Best is trial 6 with value: 0.9040640349620729.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:25,904] Trial 8 finished with value: 0.9043294329579029 and parameters: {'max_depth': 3, 'learning_rate': 0.0922235909414116}. Best is trial 8 with value: 0.9043294329579029.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:28,088] Trial 9 finished with value: 0.8838032329432165 and parameters: {'max_depth': 10, 'learning_rate': 0.0010279466353192697}. Best is trial 8 with value: 0.9043294329579029.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:28,700] Trial 10 finished with value: 0.9060694440180358 and parameters: {'max_depth': 3, 'learning_rate': 0.17455978581702686}. Best is trial 10 with value: 0.9060694440180358.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:29] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:29,251] Trial 11 finished with value: 0.9054501054872949 and parameters: {'max_depth': 2, 'learning_rate': 0.2976915190132917}. Best is trial 10 with value: 0.9060694440180358.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:29] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:29] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:29] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:30,131] Trial 12 finished with value: 0.9064528047966077 and parameters: {'max_depth': 5, 'learning_rate': 0.20787168609889528}. Best is trial 12 with value: 0.9064528047966077.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:31,044] Trial 13 finished with value: 0.9072195602750966 and parameters: {'max_depth': 5, 'learning_rate': 0.11612509071499869}. Best is trial 13 with value: 0.9072195602750966.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:32,100] Trial 14 finished with value: 0.9068952043719042 and parameters: {'max_depth': 6, 'learning_rate': 0.11312918853124382}. Best is trial 13 with value: 0.9072195602750966.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:32] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:32] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:32] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:33,171] Trial 15 finished with value: 0.9042704254731894 and parameters: {'max_depth': 6, 'learning_rate': 0.034487069771841385}. Best is trial 13 with value: 0.9072195602750966.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:33] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:33] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:34] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:34,457] Trial 16 finished with value: 0.9055680343486916 and parameters: {'max_depth': 7, 'learning_rate': 0.0918830694530831}. Best is trial 13 with value: 0.9072195602750966.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:34] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:37,346] Trial 17 finished with value: 0.8838032329432165 and parameters: {'max_depth': 11, 'learning_rate': 0.005708165320942511}. Best is trial 13 with value: 0.9072195602750966.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:38,223] Trial 18 finished with value: 0.9063938103585653 and parameters: {'max_depth': 5, 'learning_rate': 0.10792728673081961}. Best is trial 13 with value: 0.9072195602750966.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:38] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:38] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:39] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:39,515] Trial 19 finished with value: 0.9047422557294835 and parameters: {'max_depth': 7, 'learning_rate': 0.03502590612614299}. Best is trial 13 with value: 0.9072195602750966.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:39] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:39] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:40] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:40,260] Trial 20 finished with value: 0.9064528465459557 and parameters: {'max_depth': 4, 'learning_rate': 0.11612641283897122}. Best is trial 13 with value: 0.9072195602750966.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:40] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:40] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:40] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:40,982] Trial 21 finished with value: 0.9064823033202959 and parameters: {'max_depth': 4, 'learning_rate': 0.12342787865931722}. Best is trial 13 with value: 0.9072195602750966.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:42,038] Trial 22 finished with value: 0.905361557729545 and parameters: {'max_depth': 6, 'learning_rate': 0.05874119111710244}. Best is trial 13 with value: 0.9072195602750966.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:42,793] Trial 23 finished with value: 0.9073080949861753 and parameters: {'max_depth': 4, 'learning_rate': 0.29695103400379336}. Best is trial 23 with value: 0.9073080949861753.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:43,972] Trial 24 finished with value: 0.9038870542572804 and parameters: {'max_depth': 7, 'learning_rate': 0.2840245978724856}. Best is trial 23 with value: 0.9073080949861753.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:44,723] Trial 25 finished with value: 0.9065412377436504 and parameters: {'max_depth': 4, 'learning_rate': 0.19604035825999303}. Best is trial 23 with value: 0.9073080949861753.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:45,776] Trial 26 finished with value: 0.9056859866940966 and parameters: {'max_depth': 6, 'learning_rate': 0.06692073364832389}. Best is trial 23 with value: 0.9073080949861753.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:46,428] Trial 27 finished with value: 0.9057155426231384 and parameters: {'max_depth': 3, 'learning_rate': 0.16156246441202352}. Best is trial 23 with value: 0.9073080949861753.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:47,932] Trial 28 finished with value: 0.9045358078130138 and parameters: {'max_depth': 8, 'learning_rate': 0.030954831217621694}. Best is trial 23 with value: 0.9073080949861753.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:49] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:50,210] Trial 29 finished with value: 0.9035036439013578 and parameters: {'max_depth': 11, 'learning_rate': 0.06885878254670393}. Best is trial 23 with value: 0.9073080949861753.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:51,194] Trial 30 finished with value: 0.9071015922736861 and parameters: {'max_depth': 5, 'learning_rate': 0.13673938299615926}. Best is trial 23 with value: 0.9073080949861753.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:52,062] Trial 31 finished with value: 0.9052436079934743 and parameters: {'max_depth': 5, 'learning_rate': 0.2979123019843892}. Best is trial 23 with value: 0.9073080949861753.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:52] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:52] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:52] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:52,933] Trial 32 finished with value: 0.9063643013975401 and parameters: {'max_depth': 4, 'learning_rate': 0.13854727097051725}. Best is trial 23 with value: 0.9073080949861753.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:53] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:53] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:53] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:54,349] Trial 33 finished with value: 0.9050371496396674 and parameters: {'max_depth': 8, 'learning_rate': 0.05251185174918309}. Best is trial 23 with value: 0.9073080949861753.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:54,931] Trial 34 finished with value: 0.900554475178105 and parameters: {'max_depth': 2, 'learning_rate': 0.09121100220044477}. Best is trial 23 with value: 0.9073080949861753.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:55] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:55] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:56] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:56,960] Trial 35 finished with value: 0.9022060141511815 and parameters: {'max_depth': 16, 'learning_rate': 0.2340682232796004}. Best is trial 23 with value: 0.9073080949861753.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:56] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:57] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:57] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:57,510] Trial 36 finished with value: 0.8875486817619889 and parameters: {'max_depth': 6, 'learning_rate': 0.008943692481812909}. Best is trial 23 with value: 0.9073080949861753.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:57] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:57] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:57] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:56:57,792] Trial 37 finished with value: 0.8838032329432165 and parameters: {'max_depth': 3, 'learning_rate': 0.0022414661843684763}. Best is trial 23 with value: 0.9073080949861753.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:57] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:58] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:56:59] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:00,369] Trial 38 finished with value: 0.9009968173480482 and parameters: {'max_depth': 14, 'learning_rate': 0.019380284780431474}. Best is trial 23 with value: 0.9073080949861753.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:00] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:00] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:00] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:00,928] Trial 39 finished with value: 0.9060104260959853 and parameters: {'max_depth': 7, 'learning_rate': 0.14495927269343323}. Best is trial 23 with value: 0.9073080949861753.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:00] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:01,609] Trial 40 finished with value: 0.905774435297145 and parameters: {'max_depth': 8, 'learning_rate': 0.07353071626577273}. Best is trial 23 with value: 0.9073080949861753.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:01,958] Trial 41 finished with value: 0.9068362073245275 and parameters: {'max_depth': 4, 'learning_rate': 0.1778841672183709}. Best is trial 23 with value: 0.9073080949861753.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:02,360] Trial 42 finished with value: 0.9060104078306455 and parameters: {'max_depth': 5, 'learning_rate': 0.22648997880514718}. Best is trial 23 with value: 0.9073080949861753.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:02,703] Trial 43 finished with value: 0.90373961121619 and parameters: {'max_depth': 4, 'learning_rate': 0.044138112896618245}. Best is trial 23 with value: 0.9073080949861753.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:03,005] Trial 44 finished with value: 0.9060694440180358 and parameters: {'max_depth': 3, 'learning_rate': 0.16637976615610725}. Best is trial 23 with value: 0.9073080949861753.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:03] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:03] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:03] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:03,301] Trial 45 finished with value: 0.900200576392542 and parameters: {'max_depth': 2, 'learning_rate': 0.08921973592606693}. Best is trial 23 with value: 0.9073080949861753.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:03] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:03] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:03] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:03,755] Trial 46 finished with value: 0.9042999266062118 and parameters: {'max_depth': 6, 'learning_rate': 0.2423025994907272}. Best is trial 23 with value: 0.9073080949861753.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:03] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:03] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:04,184] Trial 47 finished with value: 0.9071606049770681 and parameters: {'max_depth': 5, 'learning_rate': 0.1296203222107172}. Best is trial 23 with value: 0.9073080949861753.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:04,595] Trial 48 finished with value: 0.9076029601936823 and parameters: {'max_depth': 5, 'learning_rate': 0.12639275017555568}. Best is trial 48 with value: 0.9076029601936823.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:04,978] Trial 49 finished with value: 0.9018816295453123 and parameters: {'max_depth': 5, 'learning_rate': 0.02553615613910813}. Best is trial 48 with value: 0.9076029601936823.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:05,275] Trial 50 finished with value: 0.9000826214378028 and parameters: {'max_depth': 3, 'learning_rate': 0.043559455141275925}. Best is trial 48 with value: 0.9076029601936823.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:05,672] Trial 51 finished with value: 0.9069246428809046 and parameters: {'max_depth': 5, 'learning_rate': 0.11034877468620248}. Best is trial 48 with value: 0.9076029601936823.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:06,129] Trial 52 finished with value: 0.9064232958355823 and parameters: {'max_depth': 5, 'learning_rate': 0.0834217344720229}. Best is trial 48 with value: 0.9076029601936823.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:06,525] Trial 53 finished with value: 0.9071015792270148 and parameters: {'max_depth': 5, 'learning_rate': 0.12686553978229848}. Best is trial 48 with value: 0.9076029601936823.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:06,879] Trial 54 finished with value: 0.9069836581936208 and parameters: {'max_depth': 4, 'learning_rate': 0.13013642274672277}. Best is trial 48 with value: 0.9076029601936823.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:07,370] Trial 55 finished with value: 0.9057450281001554 and parameters: {'max_depth': 6, 'learning_rate': 0.1990336882727785}. Best is trial 48 with value: 0.9076029601936823.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:07,930] Trial 56 finished with value: 0.8959538092311243 and parameters: {'max_depth': 7, 'learning_rate': 0.012542553390972284}. Best is trial 48 with value: 0.9076029601936823.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:08] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:08] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:08,673] Trial 57 finished with value: 0.904712744159124 and parameters: {'max_depth': 9, 'learning_rate': 0.14747479904758584}. Best is trial 48 with value: 0.9076029601936823.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:08] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:08] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:08] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:09,010] Trial 58 finished with value: 0.9046243373054237 and parameters: {'max_depth': 3, 'learning_rate': 0.10128852339527297}. Best is trial 48 with value: 0.9076029601936823.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:09,391] Trial 59 finished with value: 0.9074555275899288 and parameters: {'max_depth': 4, 'learning_rate': 0.25104205143064307}. Best is trial 48 with value: 0.9076029601936823.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:10] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:10,267] Trial 60 finished with value: 0.9016162211121452 and parameters: {'max_depth': 10, 'learning_rate': 0.2551167814887161}. Best is trial 48 with value: 0.9076029601936823.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:10] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:10] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:10] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:10,706] Trial 61 finished with value: 0.9069541231392529 and parameters: {'max_depth': 5, 'learning_rate': 0.19490052443936037}. Best is trial 48 with value: 0.9076029601936823.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:10] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:10] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:10] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:11,024] Trial 62 finished with value: 0.9076324691547075 and parameters: {'max_depth': 4, 'learning_rate': 0.16103860288557242}. Best is trial 62 with value: 0.9076324691547075.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:11] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:11] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:11] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:11,413] Trial 63 finished with value: 0.9063643431468881 and parameters: {'max_depth': 4, 'learning_rate': 0.2523891039510729}. Best is trial 62 with value: 0.9076324691547075.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:11] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:11] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:11] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:11,730] Trial 64 finished with value: 0.9061284097534013 and parameters: {'max_depth': 3, 'learning_rate': 0.15961415096634785}. Best is trial 62 with value: 0.9076324691547075.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:11] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:11] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:11] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:12,007] Trial 65 finished with value: 0.8991388565518443 and parameters: {'max_depth': 2, 'learning_rate': 0.07859612383409637}. Best is trial 62 with value: 0.9076324691547075.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:12] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:12] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:12] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:12,432] Trial 66 finished with value: 0.9047717777371801 and parameters: {'max_depth': 4, 'learning_rate': 0.057429856477606685}. Best is trial 62 with value: 0.9076324691547075.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:12] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:12] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:12] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:12,929] Trial 67 finished with value: 0.9036216719174561 and parameters: {'max_depth': 6, 'learning_rate': 0.29398542917497866}. Best is trial 62 with value: 0.9076324691547075.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:12] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:13,245] Trial 68 finished with value: 0.9069836868962975 and parameters: {'max_depth': 3, 'learning_rate': 0.19508057350597763}. Best is trial 62 with value: 0.9076324691547075.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:13,672] Trial 69 finished with value: 0.9068361812311849 and parameters: {'max_depth': 5, 'learning_rate': 0.12412963676828914}. Best is trial 62 with value: 0.9076324691547075.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:14,009] Trial 70 finished with value: 0.9068951808878958 and parameters: {'max_depth': 4, 'learning_rate': 0.2148303027412465}. Best is trial 62 with value: 0.9076324691547075.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:14,457] Trial 71 finished with value: 0.9066592501037433 and parameters: {'max_depth': 5, 'learning_rate': 0.13279641160205613}. Best is trial 62 with value: 0.9076324691547075.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:14,936] Trial 72 finished with value: 0.9071900643607426 and parameters: {'max_depth': 6, 'learning_rate': 0.11029757133569532}. Best is trial 62 with value: 0.9076324691547075.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:15] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:15] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:15,487] Trial 73 finished with value: 0.9066002530563667 and parameters: {'max_depth': 6, 'learning_rate': 0.10943300500672293}. Best is trial 62 with value: 0.9076324691547075.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:15] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:15] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:15] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:16,105] Trial 74 finished with value: 0.9054206382756176 and parameters: {'max_depth': 7, 'learning_rate': 0.17088510357205272}. Best is trial 62 with value: 0.9076324691547075.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:16] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:16] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:16] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:16,539] Trial 75 finished with value: 0.9072785547131389 and parameters: {'max_depth': 4, 'learning_rate': 0.15166716766227334}. Best is trial 62 with value: 0.9076324691547075.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:16] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:16] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:16] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:17,005] Trial 76 finished with value: 0.9066592214010664 and parameters: {'max_depth': 4, 'learning_rate': 0.10294578468731705}. Best is trial 62 with value: 0.9076324691547075.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:17,427] Trial 77 finished with value: 0.9065118514213348 and parameters: {'max_depth': 3, 'learning_rate': 0.26997844312460434}. Best is trial 62 with value: 0.9076324691547075.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:17,956] Trial 78 finished with value: 0.8838032329432165 and parameters: {'max_depth': 4, 'learning_rate': 0.0031125062578032407}. Best is trial 62 with value: 0.9076324691547075.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:18] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:18] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:18] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:18,266] Trial 79 finished with value: 0.9038575896549373 and parameters: {'max_depth': 2, 'learning_rate': 0.15497091703485216}. Best is trial 62 with value: 0.9076324691547075.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:18] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:18] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:18] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:18,882] Trial 80 finished with value: 0.9066002869777119 and parameters: {'max_depth': 6, 'learning_rate': 0.1807858527733933}. Best is trial 62 with value: 0.9076324691547075.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:18] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:19] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:19] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:19,253] Trial 81 finished with value: 0.9058629856642288 and parameters: {'max_depth': 4, 'learning_rate': 0.21836667320037412}. Best is trial 62 with value: 0.9076324691547075.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:19] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:19] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:19] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:19,735] Trial 82 finished with value: 0.906983639928281 and parameters: {'max_depth': 5, 'learning_rate': 0.0943897466223134}. Best is trial 62 with value: 0.9076324691547075.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:19] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:19] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:20] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:20,268] Trial 83 finished with value: 0.9057744509531503 and parameters: {'max_depth': 5, 'learning_rate': 0.14260701041703291}. Best is trial 62 with value: 0.9076324691547075.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:20] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:20] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:20] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:20,824] Trial 84 finished with value: 0.9056270287867338 and parameters: {'max_depth': 6, 'learning_rate': 0.06525684047766472}. Best is trial 62 with value: 0.9076324691547075.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:20] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:20] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:21,213] Trial 85 finished with value: 0.9068066905354994 and parameters: {'max_depth': 4, 'learning_rate': 0.11380476746937737}. Best is trial 62 with value: 0.9076324691547075.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:21,574] Trial 86 finished with value: 0.9030907898177661 and parameters: {'max_depth': 3, 'learning_rate': 0.0759918094090979}. Best is trial 62 with value: 0.9076324691547075.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:22,051] Trial 87 finished with value: 0.905951452532617 and parameters: {'max_depth': 5, 'learning_rate': 0.23765087400132842}. Best is trial 62 with value: 0.9076324691547075.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:22] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:22] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:22] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:22,483] Trial 88 finished with value: 0.9074260081915666 and parameters: {'max_depth': 4, 'learning_rate': 0.1819742367327651}. Best is trial 62 with value: 0.9076324691547075.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:22] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:22] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:22] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:22,862] Trial 89 finished with value: 0.9060104861106729 and parameters: {'max_depth': 3, 'learning_rate': 0.17260540209213407}. Best is trial 62 with value: 0.9076324691547075.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:22] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:23] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:23] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:23,344] Trial 90 finished with value: 0.9071016366323684 and parameters: {'max_depth': 4, 'learning_rate': 0.29751591066435223}. Best is trial 62 with value: 0.9076324691547075.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:23] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:23] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:23] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:23,775] Trial 91 finished with value: 0.9078389066338405 and parameters: {'max_depth': 4, 'learning_rate': 0.2927776471734514}. Best is trial 91 with value: 0.9078389066338405.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:23] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:23] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:24] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:24,207] Trial 92 finished with value: 0.9074850443789567 and parameters: {'max_depth': 4, 'learning_rate': 0.2150246044490188}. Best is trial 91 with value: 0.9078389066338405.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:24] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:24] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:24] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:24,618] Trial 93 finished with value: 0.9070721433273486 and parameters: {'max_depth': 4, 'learning_rate': 0.22015809688139326}. Best is trial 91 with value: 0.9078389066338405.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:24] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:26,007] Trial 94 finished with value: 0.9016161219574439 and parameters: {'max_depth': 13, 'learning_rate': 0.25860801389686044}. Best is trial 91 with value: 0.9078389066338405.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:26,483] Trial 95 finished with value: 0.9066887616741027 and parameters: {'max_depth': 4, 'learning_rate': 0.19432881902070495}. Best is trial 91 with value: 0.9078389066338405.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:26,828] Trial 96 finished with value: 0.9058925050625911 and parameters: {'max_depth': 3, 'learning_rate': 0.1567786867157862}. Best is trial 91 with value: 0.9078389066338405.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:27,207] Trial 97 finished with value: 0.9075144933252942 and parameters: {'max_depth': 4, 'learning_rate': 0.22630810765390694}. Best is trial 91 with value: 0.9078389066338405.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:27,542] Trial 98 finished with value: 0.905715579153818 and parameters: {'max_depth': 2, 'learning_rate': 0.2652504061861836}. Best is trial 91 with value: 0.9078389066338405.\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "C:\\Users\\Gulsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [15:57:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-15 15:57:27,924] Trial 99 finished with value: 0.9075439918489826 and parameters: {'max_depth': 4, 'learning_rate': 0.22890012398217657}. Best is trial 91 with value: 0.9078389066338405.\n"
          ]
        }
      ],
      "source": [
        "xgb_study = run_study(xgb_objective, \"XGBoost\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:27,936] A new study created in memory with name: LightGBM\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001303 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001041 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:28,295] Trial 0 finished with value: 0.9058629934922316 and parameters: {'max_depth': 4, 'learning_rate': 0.09700048944379512}. Best is trial 0 with value: 0.9058629934922316.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002514 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001248 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001048 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001136 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:28,689] Trial 1 finished with value: 0.8839801901640008 and parameters: {'max_depth': 5, 'learning_rate': 0.008366693937247472}. Best is trial 0 with value: 0.9058629934922316.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001033 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001033 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001063 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:29,109] Trial 2 finished with value: 0.9048602680895762 and parameters: {'max_depth': 12, 'learning_rate': 0.23275862219997764}. Best is trial 0 with value: 0.9058629934922316.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001000 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000965 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:29,599] Trial 3 finished with value: 0.8838032329432165 and parameters: {'max_depth': 13, 'learning_rate': 0.006502315576819135}. Best is trial 0 with value: 0.9058629934922316.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000973 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001166 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001066 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001137 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:30,095] Trial 4 finished with value: 0.9053615942602246 and parameters: {'max_depth': 12, 'learning_rate': 0.025809711311765904}. Best is trial 0 with value: 0.9058629934922316.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001026 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001183 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:30,426] Trial 5 finished with value: 0.9002006024858846 and parameters: {'max_depth': 4, 'learning_rate': 0.0260649934482151}. Best is trial 0 with value: 0.9058629934922316.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001189 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001123 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001004 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001102 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:30,935] Trial 6 finished with value: 0.8838032329432165 and parameters: {'max_depth': 14, 'learning_rate': 0.0030003979392253543}. Best is trial 0 with value: 0.9058629934922316.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001115 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001070 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:31,398] Trial 7 finished with value: 0.8838032329432165 and parameters: {'max_depth': 7, 'learning_rate': 0.0018319062870564165}. Best is trial 0 with value: 0.9058629934922316.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000988 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000980 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001105 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001073 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:31,817] Trial 8 finished with value: 0.8866639087047385 and parameters: {'max_depth': 6, 'learning_rate': 0.01037355687163963}. Best is trial 0 with value: 0.9058629934922316.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001726 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001165 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:32,342] Trial 9 finished with value: 0.9026778470168099 and parameters: {'max_depth': 15, 'learning_rate': 0.01931600677567005}. Best is trial 0 with value: 0.9058629934922316.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001035 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001068 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:32,566] Trial 10 finished with value: 0.9033267336485901 and parameters: {'max_depth': 2, 'learning_rate': 0.1461615450040976}. Best is trial 0 with value: 0.9058629934922316.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001044 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000969 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001034 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000972 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000937 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:33,051] Trial 11 finished with value: 0.9082517763734375 and parameters: {'max_depth': 10, 'learning_rate': 0.06385973635329044}. Best is trial 11 with value: 0.9082517763734375.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000983 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001187 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:33,515] Trial 12 finished with value: 0.9080747826219738 and parameters: {'max_depth': 9, 'learning_rate': 0.07846713272308815}. Best is trial 11 with value: 0.9082517763734375.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000929 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001186 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000987 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001030 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:33,956] Trial 13 finished with value: 0.9073670059255216 and parameters: {'max_depth': 9, 'learning_rate': 0.07528019968522594}. Best is trial 11 with value: 0.9082517763734375.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000926 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001074 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:34,431] Trial 14 finished with value: 0.9079863235815884 and parameters: {'max_depth': 10, 'learning_rate': 0.053518573460530926}. Best is trial 11 with value: 0.9082517763734375.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001049 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000985 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001020 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000975 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:34,925] Trial 15 finished with value: 0.9073080297528189 and parameters: {'max_depth': 10, 'learning_rate': 0.042032235117483836}. Best is trial 11 with value: 0.9082517763734375.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001085 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001030 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:35,363] Trial 16 finished with value: 0.9055680186926861 and parameters: {'max_depth': 8, 'learning_rate': 0.2469354112956401}. Best is trial 11 with value: 0.9082517763734375.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000971 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001555 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001134 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001414 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:35,807] Trial 17 finished with value: 0.9069246611462444 and parameters: {'max_depth': 11, 'learning_rate': 0.12297437712056564}. Best is trial 11 with value: 0.9082517763734375.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001028 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000971 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:36,275] Trial 18 finished with value: 0.9084876862829162 and parameters: {'max_depth': 16, 'learning_rate': 0.05366711954814592}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001004 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001203 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001022 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001161 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:36,751] Trial 19 finished with value: 0.908399208977191 and parameters: {'max_depth': 16, 'learning_rate': 0.04563856360594272}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001125 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001072 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:37,278] Trial 20 finished with value: 0.9075734460139886 and parameters: {'max_depth': 15, 'learning_rate': 0.03575705207838966}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001336 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001366 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001445 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:37,794] Trial 21 finished with value: 0.8934764733735001 and parameters: {'max_depth': 16, 'learning_rate': 0.014062280012078475}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001215 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001131 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001038 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000995 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:38,269] Trial 22 finished with value: 0.9079273369715489 and parameters: {'max_depth': 16, 'learning_rate': 0.054576295890515304}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000926 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001191 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:38,738] Trial 23 finished with value: 0.9052730934704912 and parameters: {'max_depth': 14, 'learning_rate': 0.17380867233095323}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005278 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001175 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001021 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002859 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:39,251] Trial 24 finished with value: 0.9060103973933084 and parameters: {'max_depth': 13, 'learning_rate': 0.030581418897988974}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001043 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001152 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:39,765] Trial 25 finished with value: 0.9074849478335896 and parameters: {'max_depth': 16, 'learning_rate': 0.05591906019261216}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000977 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001074 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001039 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000935 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:40,259] Trial 26 finished with value: 0.8971629512382385 and parameters: {'max_depth': 14, 'learning_rate': 0.015865319912639602}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001045 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000995 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:40,717] Trial 27 finished with value: 0.9077209203670901 and parameters: {'max_depth': 12, 'learning_rate': 0.0946719901981391}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001061 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000965 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001097 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000985 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:41,205] Trial 28 finished with value: 0.8838032329432165 and parameters: {'max_depth': 15, 'learning_rate': 0.0051569176749890035}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001034 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001107 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000934 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:41,426] Trial 29 finished with value: 0.9030023464333863 and parameters: {'max_depth': 2, 'learning_rate': 0.12687156753408343}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001415 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001053 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:41,955] Trial 30 finished with value: 0.9075144881066258 and parameters: {'max_depth': 7, 'learning_rate': 0.0688988921791332}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001011 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001068 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001026 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000964 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:42,402] Trial 31 finished with value: 0.9078978906345457 and parameters: {'max_depth': 9, 'learning_rate': 0.09098463376670786}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000938 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000976 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:42,892] Trial 32 finished with value: 0.9080453101916279 and parameters: {'max_depth': 10, 'learning_rate': 0.04308782412928668}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000918 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001023 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001175 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000968 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:43,353] Trial 33 finished with value: 0.8838032329432165 and parameters: {'max_depth': 8, 'learning_rate': 0.0010085484239712034}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001013 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001075 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001031 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:43,794] Trial 34 finished with value: 0.9066887303620916 and parameters: {'max_depth': 11, 'learning_rate': 0.2035470049661584}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001154 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001044 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:44,251] Trial 35 finished with value: 0.9003480637923148 and parameters: {'max_depth': 5, 'learning_rate': 0.02067070543058491}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002668 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000941 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001117 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:44,729] Trial 36 finished with value: 0.9081337979346901 and parameters: {'max_depth': 13, 'learning_rate': 0.07080964880107359}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001039 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001064 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001131 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000987 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:45,201] Trial 37 finished with value: 0.9061283575667164 and parameters: {'max_depth': 13, 'learning_rate': 0.028226155455859082}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001170 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002497 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:45,697] Trial 38 finished with value: 0.9067476908787887 and parameters: {'max_depth': 13, 'learning_rate': 0.10454895555004107}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001482 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001074 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001024 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001673 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:46,195] Trial 39 finished with value: 0.9083696869694946 and parameters: {'max_depth': 15, 'learning_rate': 0.06018511821236333}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000998 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002764 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:46,628] Trial 40 finished with value: 0.9024714330216854 and parameters: {'max_depth': 15, 'learning_rate': 0.28045472411756395}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000991 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001033 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000969 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001015 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:47,105] Trial 41 finished with value: 0.9073374917458277 and parameters: {'max_depth': 14, 'learning_rate': 0.04089188037378892}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001103 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001002 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:47,568] Trial 42 finished with value: 0.907868397329526 and parameters: {'max_depth': 16, 'learning_rate': 0.06259679516080634}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000952 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000901 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001105 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000969 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:48,055] Trial 43 finished with value: 0.9053910692999046 and parameters: {'max_depth': 15, 'learning_rate': 0.02452118356645771}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000971 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001026 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:48,513] Trial 44 finished with value: 0.9067476491294407 and parameters: {'max_depth': 12, 'learning_rate': 0.16245089052602538}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000944 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001094 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001063 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001051 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:48,996] Trial 45 finished with value: 0.9075439422716318 and parameters: {'max_depth': 14, 'learning_rate': 0.04795548762979058}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000979 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001128 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:49,472] Trial 46 finished with value: 0.9068951547945533 and parameters: {'max_depth': 16, 'learning_rate': 0.08721067949523745}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001049 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001064 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001042 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000992 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:49,976] Trial 47 finished with value: 0.8927096970203371 and parameters: {'max_depth': 13, 'learning_rate': 0.013030975321257446}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001081 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001066 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001185 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:50,591] Trial 48 finished with value: 0.9065117392199622 and parameters: {'max_depth': 15, 'learning_rate': 0.03272432486983792}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001108 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001048 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:51,051] Trial 49 finished with value: 0.9081927975914007 and parameters: {'max_depth': 11, 'learning_rate': 0.07442435893067169}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001113 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001038 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001080 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001497 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:51,604] Trial 50 finished with value: 0.904889709207911 and parameters: {'max_depth': 11, 'learning_rate': 0.023111288444002513}. Best is trial 18 with value: 0.9084876862829162.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001100 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001198 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:52,115] Trial 51 finished with value: 0.9085761844633152 and parameters: {'max_depth': 11, 'learning_rate': 0.06733966863133908}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001001 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001072 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000968 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001035 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:52,541] Trial 52 finished with value: 0.9064528074059418 and parameters: {'max_depth': 10, 'learning_rate': 0.12038570162142744}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001117 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001172 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:53,016] Trial 53 finished with value: 0.9080748374179929 and parameters: {'max_depth': 11, 'learning_rate': 0.06464561813390447}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001079 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000970 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001006 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000930 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:53,490] Trial 54 finished with value: 0.9072785234011279 and parameters: {'max_depth': 12, 'learning_rate': 0.03743888216549403}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001002 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001148 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:53,946] Trial 55 finished with value: 0.907396496621207 and parameters: {'max_depth': 9, 'learning_rate': 0.07899867873979073}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001002 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001077 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001037 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000974 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:54,412] Trial 56 finished with value: 0.9069541231392529 and parameters: {'max_depth': 8, 'learning_rate': 0.05050230719761697}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001061 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001107 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:54,887] Trial 57 finished with value: 0.9061284071440671 and parameters: {'max_depth': 16, 'learning_rate': 0.13997596106069493}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001004 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001215 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001008 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000933 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:55,347] Trial 58 finished with value: 0.9075734512326571 and parameters: {'max_depth': 10, 'learning_rate': 0.09937824930442943}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001041 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000936 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000912 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:55,609] Trial 59 finished with value: 0.902058571110091 and parameters: {'max_depth': 3, 'learning_rate': 0.05813617901994616}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001049 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001043 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:56,091] Trial 60 finished with value: 0.8853663024385705 and parameters: {'max_depth': 14, 'learning_rate': 0.008527850415150067}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001535 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001078 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001091 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000968 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:56,542] Trial 61 finished with value: 0.9076619024450396 and parameters: {'max_depth': 12, 'learning_rate': 0.07338887633342922}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000933 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001023 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:57,021] Trial 62 finished with value: 0.9073375204485045 and parameters: {'max_depth': 15, 'learning_rate': 0.051320374376286976}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001061 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001094 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001074 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001029 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:57,515] Trial 63 finished with value: 0.907485036550954 and parameters: {'max_depth': 11, 'learning_rate': 0.10618479953814802}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001033 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000996 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:58,008] Trial 64 finished with value: 0.9073965070585439 and parameters: {'max_depth': 14, 'learning_rate': 0.0433972868935927}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000942 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001290 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001037 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000952 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:58,475] Trial 65 finished with value: 0.9075144698412861 and parameters: {'max_depth': 13, 'learning_rate': 0.07939223350212786}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001167 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001081 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:59,027] Trial 66 finished with value: 0.9071605762743914 and parameters: {'max_depth': 16, 'learning_rate': 0.03550007238617895}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001079 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001060 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000995 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001069 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:57:59,537] Trial 67 finished with value: 0.9060693683473425 and parameters: {'max_depth': 11, 'learning_rate': 0.0295104812659003}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001129 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001414 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:00,017] Trial 68 finished with value: 0.9079863366282597 and parameters: {'max_depth': 10, 'learning_rate': 0.06811220638273457}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000999 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001139 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001068 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001740 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:00,450] Trial 69 finished with value: 0.9060103686906317 and parameters: {'max_depth': 12, 'learning_rate': 0.17412691303948422}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000905 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000978 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:00,949] Trial 70 finished with value: 0.9074554727939095 and parameters: {'max_depth': 15, 'learning_rate': 0.06118170371218289}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000936 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002002 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000924 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000902 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:01,418] Trial 71 finished with value: 0.9062758527944919 and parameters: {'max_depth': 9, 'learning_rate': 0.11391315263367721}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001146 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001468 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001009 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:01,926] Trial 72 finished with value: 0.908104260270988 and parameters: {'max_depth': 11, 'learning_rate': 0.06366650466065621}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001090 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001102 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:02,383] Trial 73 finished with value: 0.9082222908964205 and parameters: {'max_depth': 10, 'learning_rate': 0.08360185855647313}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000986 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001042 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001053 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001056 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:02,852] Trial 74 finished with value: 0.9077799174144667 and parameters: {'max_depth': 10, 'learning_rate': 0.04306557677838601}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000992 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001102 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:03,289] Trial 75 finished with value: 0.9075734642793284 and parameters: {'max_depth': 9, 'learning_rate': 0.09053068058916504}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001019 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001019 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001194 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001025 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:03,759] Trial 76 finished with value: 0.908517190025273 and parameters: {'max_depth': 16, 'learning_rate': 0.07929502605476288}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001225 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001013 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:04,239] Trial 77 finished with value: 0.906983639928281 and parameters: {'max_depth': 16, 'learning_rate': 0.1392888340176802}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001110 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000979 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001079 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000997 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:04,674] Trial 78 finished with value: 0.9073670346281982 and parameters: {'max_depth': 8, 'learning_rate': 0.08292242584954425}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001048 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001093 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:05,086] Trial 79 finished with value: 0.9058334688752009 and parameters: {'max_depth': 7, 'learning_rate': 0.19796848689642554}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001010 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001128 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000970 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:05,541] Trial 80 finished with value: 0.9083402301951544 and parameters: {'max_depth': 16, 'learning_rate': 0.04811620801302714}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000998 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001073 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001054 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002150 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:06,074] Trial 81 finished with value: 0.9078683477521752 and parameters: {'max_depth': 16, 'learning_rate': 0.04961738669121694}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001184 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000932 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:06,553] Trial 82 finished with value: 0.9082812435851149 and parameters: {'max_depth': 15, 'learning_rate': 0.05559532161419572}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000904 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001085 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000933 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000961 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:07,057] Trial 83 finished with value: 0.9069246428809046 and parameters: {'max_depth': 15, 'learning_rate': 0.03580837613997918}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001005 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001078 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:07,535] Trial 84 finished with value: 0.9083991750558459 and parameters: {'max_depth': 16, 'learning_rate': 0.05515518620592663}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001839 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001062 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001019 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001069 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:08,025] Trial 85 finished with value: 0.9079273239248776 and parameters: {'max_depth': 16, 'learning_rate': 0.05594486169283276}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000996 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001130 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:08,537] Trial 86 finished with value: 0.8838032329432165 and parameters: {'max_depth': 15, 'learning_rate': 0.004713742273959251}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001116 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001271 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000942 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000922 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:09,025] Trial 87 finished with value: 0.9080157699185918 and parameters: {'max_depth': 16, 'learning_rate': 0.0455868538773589}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000998 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001133 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:09,573] Trial 88 finished with value: 0.9017635989198797 and parameters: {'max_depth': 15, 'learning_rate': 0.018880133521195143}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000973 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001017 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001112 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000931 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:10,079] Trial 89 finished with value: 0.908015835151948 and parameters: {'max_depth': 16, 'learning_rate': 0.03905965413321408}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000986 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001296 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:10,588] Trial 90 finished with value: 0.9078683581895121 and parameters: {'max_depth': 14, 'learning_rate': 0.054722954562894194}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001015 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001203 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000993 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001354 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:11,027] Trial 91 finished with value: 0.9074259977542295 and parameters: {'max_depth': 15, 'learning_rate': 0.09773354434853199}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000962 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000937 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:11,488] Trial 92 finished with value: 0.907897851494532 and parameters: {'max_depth': 16, 'learning_rate': 0.06732658461697681}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000923 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001082 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001042 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000963 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:11,967] Trial 93 finished with value: 0.9066002139163528 and parameters: {'max_depth': 16, 'learning_rate': 0.08363005692935095}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001022 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000944 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:12,420] Trial 94 finished with value: 0.9085171821972701 and parameters: {'max_depth': 15, 'learning_rate': 0.04761959510429767}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001017 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001168 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001027 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000919 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:12,945] Trial 95 finished with value: 0.9085171821972701 and parameters: {'max_depth': 15, 'learning_rate': 0.04988769269128999}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000967 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001027 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:13,423] Trial 96 finished with value: 0.9064232566955687 and parameters: {'max_depth': 15, 'learning_rate': 0.03190455202544286}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001261 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001078 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001127 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000957 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:13,897] Trial 97 finished with value: 0.9077504293281153 and parameters: {'max_depth': 14, 'learning_rate': 0.04742560159006018}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001081 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001269 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:14,403] Trial 98 finished with value: 0.9060103869559715 and parameters: {'max_depth': 15, 'learning_rate': 0.02784334846219803}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000961 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001063 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002385 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000996 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:14,963] Trial 99 finished with value: 0.9060988616523623 and parameters: {'max_depth': 16, 'learning_rate': 0.033253212330402765}. Best is trial 51 with value: 0.9085761844633152.\n"
          ]
        }
      ],
      "source": [
        "lgbm_study = run_study(lgbm_objective, \"LightGBM\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 15:58:14,973] A new study created in memory with name: GradientBoosting\n",
            "[I 2025-04-15 15:58:28,711] Trial 0 finished with value: 0.8838032329432165 and parameters: {'max_depth': 5, 'learning_rate': 0.002350553667233454}. Best is trial 0 with value: 0.8838032329432165.\n",
            "[I 2025-04-15 15:58:59,116] Trial 1 finished with value: 0.8987555140386122 and parameters: {'max_depth': 9, 'learning_rate': 0.2931721688573279}. Best is trial 1 with value: 0.8987555140386122.\n",
            "[I 2025-04-15 16:00:30,633] Trial 2 finished with value: 0.9010263106530679 and parameters: {'max_depth': 11, 'learning_rate': 0.10347205874876311}. Best is trial 2 with value: 0.9010263106530679.\n",
            "[I 2025-04-15 16:01:02,118] Trial 3 finished with value: 0.9055975380910484 and parameters: {'max_depth': 5, 'learning_rate': 0.14867755443126732}. Best is trial 3 with value: 0.9055975380910484.\n",
            "[I 2025-04-15 16:01:34,754] Trial 4 finished with value: 0.9024714356310196 and parameters: {'max_depth': 5, 'learning_rate': 0.024617207179595247}. Best is trial 3 with value: 0.9055975380910484.\n",
            "[I 2025-04-15 16:02:31,356] Trial 5 finished with value: 0.8838032329432165 and parameters: {'max_depth': 8, 'learning_rate': 0.0020091736391166177}. Best is trial 3 with value: 0.9055975380910484.\n",
            "[I 2025-04-15 16:03:05,087] Trial 6 finished with value: 0.8838032329432165 and parameters: {'max_depth': 5, 'learning_rate': 0.0014530564250479633}. Best is trial 3 with value: 0.9055975380910484.\n",
            "[I 2025-04-15 16:03:32,380] Trial 7 finished with value: 0.9058039599141757 and parameters: {'max_depth': 4, 'learning_rate': 0.08543635647176302}. Best is trial 7 with value: 0.9058039599141757.\n",
            "[I 2025-04-15 16:03:59,399] Trial 8 finished with value: 0.8838032329432165 and parameters: {'max_depth': 4, 'learning_rate': 0.0010960625272519056}. Best is trial 7 with value: 0.9058039599141757.\n",
            "[I 2025-04-15 16:05:07,233] Trial 9 finished with value: 0.9023239795432577 and parameters: {'max_depth': 9, 'learning_rate': 0.055306544601215916}. Best is trial 7 with value: 0.9058039599141757.\n",
            "[I 2025-04-15 16:07:40,410] Trial 10 finished with value: 0.8895540934272859 and parameters: {'max_depth': 15, 'learning_rate': 0.008076698432296123}. Best is trial 7 with value: 0.9058039599141757.\n",
            "[I 2025-04-15 16:07:53,647] Trial 11 finished with value: 0.9048307539098825 and parameters: {'max_depth': 2, 'learning_rate': 0.2957993060468698}. Best is trial 7 with value: 0.9058039599141757.\n",
            "[I 2025-04-15 16:08:06,967] Trial 12 finished with value: 0.9007609465785832 and parameters: {'max_depth': 2, 'learning_rate': 0.09938835849294278}. Best is trial 7 with value: 0.9058039599141757.\n",
            "[I 2025-04-15 16:08:55,395] Trial 13 finished with value: 0.9039460147739775 and parameters: {'max_depth': 7, 'learning_rate': 0.033291875648482586}. Best is trial 7 with value: 0.9058039599141757.\n",
            "[I 2025-04-15 21:33:29,071] Trial 14 finished with value: 0.9002890484795986 and parameters: {'max_depth': 13, 'learning_rate': 0.10548308825384825}. Best is trial 7 with value: 0.9058039599141757.\n",
            "[I 2025-04-15 21:33:47,364] Trial 15 finished with value: 0.8911171837973139 and parameters: {'max_depth': 7, 'learning_rate': 0.009731541074802503}. Best is trial 7 with value: 0.9058039599141757.\n",
            "[I 2025-04-15 21:33:54,220] Trial 16 finished with value: 0.9042704463478634 and parameters: {'max_depth': 3, 'learning_rate': 0.15096283193225957}. Best is trial 7 with value: 0.9058039599141757.\n",
            "[I 2025-04-15 21:34:08,383] Trial 17 finished with value: 0.9057744196411394 and parameters: {'max_depth': 6, 'learning_rate': 0.04557578478736078}. Best is trial 7 with value: 0.9058039599141757.\n",
            "[I 2025-04-15 21:34:40,127] Trial 18 finished with value: 0.90002355915707 and parameters: {'max_depth': 11, 'learning_rate': 0.04189597919711507}. Best is trial 7 with value: 0.9058039599141757.\n",
            "[I 2025-04-15 21:34:59,536] Trial 19 finished with value: 0.9002890876196122 and parameters: {'max_depth': 7, 'learning_rate': 0.015567130734432532}. Best is trial 7 with value: 0.9058039599141757.\n",
            "[I 2025-04-15 21:35:07,081] Trial 20 finished with value: 0.8838032329432165 and parameters: {'max_depth': 3, 'learning_rate': 0.004557494027462316}. Best is trial 7 with value: 0.9058039599141757.\n",
            "[I 2025-04-15 21:35:21,416] Trial 21 finished with value: 0.9051846370394402 and parameters: {'max_depth': 6, 'learning_rate': 0.05677082078359185}. Best is trial 7 with value: 0.9058039599141757.\n",
            "[I 2025-04-15 21:35:33,849] Trial 22 finished with value: 0.9067477482841423 and parameters: {'max_depth': 5, 'learning_rate': 0.1599416230650203}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:35:45,207] Trial 23 finished with value: 0.9039165032036179 and parameters: {'max_depth': 4, 'learning_rate': 0.07005384354818632}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:35:58,995] Trial 24 finished with value: 0.9034446390259786 and parameters: {'max_depth': 6, 'learning_rate': 0.17754567616535075}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:36:06,346] Trial 25 finished with value: 0.8985490739501452 and parameters: {'max_depth': 3, 'learning_rate': 0.024533250987111024}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:36:43,942] Trial 26 finished with value: 0.9011737563034927 and parameters: {'max_depth': 11, 'learning_rate': 0.08168126153588824}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:37:07,966] Trial 27 finished with value: 0.9008493951816315 and parameters: {'max_depth': 8, 'learning_rate': 0.18427631069763886}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:37:27,597] Trial 28 finished with value: 0.9054795361682926 and parameters: {'max_depth': 6, 'learning_rate': 0.04369189941964612}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:37:38,545] Trial 29 finished with value: 0.9000236530931031 and parameters: {'max_depth': 4, 'learning_rate': 0.02017922933361652}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:38:11,791] Trial 30 finished with value: 0.8965435657394809 and parameters: {'max_depth': 10, 'learning_rate': 0.012018878042035785}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:38:23,729] Trial 31 finished with value: 0.9061873859261039 and parameters: {'max_depth': 5, 'learning_rate': 0.14727119803665795}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:38:33,385] Trial 32 finished with value: 0.9062463620988063 and parameters: {'max_depth': 4, 'learning_rate': 0.2411442645777237}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:38:42,811] Trial 33 finished with value: 0.9055385697463486 and parameters: {'max_depth': 4, 'learning_rate': 0.2230051833447642}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:38:49,688] Trial 34 finished with value: 0.9042999239968776 and parameters: {'max_depth': 3, 'learning_rate': 0.1331197817700373}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:38:54,317] Trial 35 finished with value: 0.9042409713081833 and parameters: {'max_depth': 2, 'learning_rate': 0.2274558050449369}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:39:05,385] Trial 36 finished with value: 0.9058334819218721 and parameters: {'max_depth': 5, 'learning_rate': 0.10786577983046931}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:39:18,057] Trial 37 finished with value: 0.9060694622833755 and parameters: {'max_depth': 5, 'learning_rate': 0.12082953333731945}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:39:37,376] Trial 38 finished with value: 0.8999646299523839 and parameters: {'max_depth': 8, 'learning_rate': 0.2289234023343244}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:39:48,181] Trial 39 finished with value: 0.9066002608843694 and parameters: {'max_depth': 5, 'learning_rate': 0.1252007597134566}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:40:03,247] Trial 40 finished with value: 0.902648330227782 and parameters: {'max_depth': 7, 'learning_rate': 0.16610692074264782}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:40:13,909] Trial 41 finished with value: 0.9058924528759063 and parameters: {'max_depth': 5, 'learning_rate': 0.12741552164797879}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:40:24,366] Trial 42 finished with value: 0.9043883908652658 and parameters: {'max_depth': 5, 'learning_rate': 0.2918801739468553}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:40:32,900] Trial 43 finished with value: 0.9046833108687921 and parameters: {'max_depth': 4, 'learning_rate': 0.07426695090627285}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:40:45,723] Trial 44 finished with value: 0.9043293964272233 and parameters: {'max_depth': 6, 'learning_rate': 0.129552785243967}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:40:56,160] Trial 45 finished with value: 0.9043883647719232 and parameters: {'max_depth': 5, 'learning_rate': 0.21563635552095775}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:41:02,480] Trial 46 finished with value: 0.905450108096629 and parameters: {'max_depth': 3, 'learning_rate': 0.2888773435542348}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:41:20,496] Trial 47 finished with value: 0.903739548592168 and parameters: {'max_depth': 8, 'learning_rate': 0.061408463346133924}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:42:21,240] Trial 48 finished with value: 0.8983720462773362 and parameters: {'max_depth': 15, 'learning_rate': 0.0956328307898384}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:42:25,836] Trial 49 finished with value: 0.8838032329432165 and parameters: {'max_depth': 2, 'learning_rate': 0.00305334230493471}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:42:37,477] Trial 50 finished with value: 0.9035626070273891 and parameters: {'max_depth': 5, 'learning_rate': 0.034043663654418965}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:42:48,897] Trial 51 finished with value: 0.9062758867158371 and parameters: {'max_depth': 5, 'learning_rate': 0.13727181011528142}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:42:58,376] Trial 52 finished with value: 0.9055975511377197 and parameters: {'max_depth': 4, 'learning_rate': 0.11657231042172468}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:44:20,162] Trial 53 finished with value: 0.8996992319565541 and parameters: {'max_depth': 16, 'learning_rate': 0.15570565904050454}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:44:35,598] Trial 54 finished with value: 0.904830748691214 and parameters: {'max_depth': 6, 'learning_rate': 0.08820224557975125}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:44:54,345] Trial 55 finished with value: 0.9019405717966694 and parameters: {'max_depth': 7, 'learning_rate': 0.19463091839648194}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:45:05,861] Trial 56 finished with value: 0.9043294120832289 and parameters: {'max_depth': 5, 'learning_rate': 0.1503547438891403}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:45:14,329] Trial 57 finished with value: 0.9050076850373244 and parameters: {'max_depth': 4, 'learning_rate': 0.25045346825325016}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:45:20,805] Trial 58 finished with value: 0.90276637650922 and parameters: {'max_depth': 3, 'learning_rate': 0.0664829245125974}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:45:33,910] Trial 59 finished with value: 0.9053616151348985 and parameters: {'max_depth': 6, 'learning_rate': 0.10433093107338404}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:45:57,986] Trial 60 finished with value: 0.8984900534187604 and parameters: {'max_depth': 9, 'learning_rate': 0.17928924029130636}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:46:11,914] Trial 61 finished with value: 0.9058039990541893 and parameters: {'max_depth': 5, 'learning_rate': 0.13324421947321077}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:46:22,952] Trial 62 finished with value: 0.9058924580945747 and parameters: {'max_depth': 4, 'learning_rate': 0.12430626459239341}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:46:32,605] Trial 63 finished with value: 0.9046832926034524 and parameters: {'max_depth': 4, 'learning_rate': 0.07842406277731465}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:46:39,737] Trial 64 finished with value: 0.9055680734887054 and parameters: {'max_depth': 3, 'learning_rate': 0.19632105655449228}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:46:58,538] Trial 65 finished with value: 0.9042408982468243 and parameters: {'max_depth': 7, 'learning_rate': 0.049706606626269745}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:47:12,626] Trial 66 finished with value: 0.9050371600770045 and parameters: {'max_depth': 5, 'learning_rate': 0.09014730239873757}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:47:28,308] Trial 67 finished with value: 0.9033561199709054 and parameters: {'max_depth': 6, 'learning_rate': 0.25968235082052865}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:48:14,147] Trial 68 finished with value: 0.9001711170088674 and parameters: {'max_depth': 13, 'learning_rate': 0.15430354190046502}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:48:23,985] Trial 69 finished with value: 0.906039940275679 and parameters: {'max_depth': 4, 'learning_rate': 0.11669662302677603}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:48:28,745] Trial 70 finished with value: 0.8957178601816319 and parameters: {'max_depth': 2, 'learning_rate': 0.03723305913967194}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:48:38,989] Trial 71 finished with value: 0.905479583136309 and parameters: {'max_depth': 4, 'learning_rate': 0.12105026166159086}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:48:49,445] Trial 72 finished with value: 0.9054795883549774 and parameters: {'max_depth': 4, 'learning_rate': 0.10472911752889959}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:48:57,480] Trial 73 finished with value: 0.9044768786083277 and parameters: {'max_depth': 3, 'learning_rate': 0.14050353533277887}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:49:10,125] Trial 74 finished with value: 0.9057744744371586 and parameters: {'max_depth': 5, 'learning_rate': 0.18124346188073603}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:49:27,362] Trial 75 finished with value: 0.9040639566820455 and parameters: {'max_depth': 6, 'learning_rate': 0.21157361820799758}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:49:38,172] Trial 76 finished with value: 0.9054205730422612 and parameters: {'max_depth': 4, 'learning_rate': 0.11409145091673113}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:49:46,433] Trial 77 finished with value: 0.9033266997272448 and parameters: {'max_depth': 3, 'learning_rate': 0.08418443382532619}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:49:58,729] Trial 78 finished with value: 0.9039164953756152 and parameters: {'max_depth': 5, 'learning_rate': 0.25161182629538753}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:50:13,499] Trial 79 finished with value: 0.8838622247719244 and parameters: {'max_depth': 6, 'learning_rate': 0.007165519918249571}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:50:22,558] Trial 80 finished with value: 0.9039165527809687 and parameters: {'max_depth': 4, 'learning_rate': 0.05174428665651483}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:50:33,680] Trial 81 finished with value: 0.905538554090343 and parameters: {'max_depth': 5, 'learning_rate': 0.12730450613137695}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:50:44,370] Trial 82 finished with value: 0.9059219696649342 and parameters: {'max_depth': 5, 'learning_rate': 0.16545764494225426}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:50:53,001] Trial 83 finished with value: 0.9053616699309178 and parameters: {'max_depth': 4, 'learning_rate': 0.16547813456079824}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:50:59,682] Trial 84 finished with value: 0.9037396268721953 and parameters: {'max_depth': 3, 'learning_rate': 0.15040178477802174}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:51:10,370] Trial 85 finished with value: 0.9047423105255028 and parameters: {'max_depth': 5, 'learning_rate': 0.19994612007647222}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:51:23,467] Trial 86 finished with value: 0.904801211027512 and parameters: {'max_depth': 6, 'learning_rate': 0.07075926373253265}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:51:35,709] Trial 87 finished with value: 0.9055975172163744 and parameters: {'max_depth': 5, 'learning_rate': 0.09603288809749207}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:51:45,456] Trial 88 finished with value: 0.9066297881107345 and parameters: {'max_depth': 4, 'learning_rate': 0.1693478579588011}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:52:01,515] Trial 89 finished with value: 0.9028842296999237 and parameters: {'max_depth': 7, 'learning_rate': 0.1737047448261866}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:52:14,960] Trial 90 finished with value: 0.8838032329432165 and parameters: {'max_depth': 6, 'learning_rate': 0.0010950248871505837}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:52:23,662] Trial 91 finished with value: 0.9049192390436104 and parameters: {'max_depth': 4, 'learning_rate': 0.2378305040887154}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:52:33,569] Trial 92 finished with value: 0.9060989190577158 and parameters: {'max_depth': 4, 'learning_rate': 0.14136951620696134}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:52:46,565] Trial 93 finished with value: 0.9045947552830395 and parameters: {'max_depth': 5, 'learning_rate': 0.2850942089735029}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:52:54,321] Trial 94 finished with value: 0.9051257026160856 and parameters: {'max_depth': 3, 'learning_rate': 0.1418589657603368}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:53:07,534] Trial 95 finished with value: 0.9058924424385691 and parameters: {'max_depth': 5, 'learning_rate': 0.10748389021854304}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:53:18,260] Trial 96 finished with value: 0.9063053852395253 and parameters: {'max_depth': 4, 'learning_rate': 0.20902768451305617}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:53:28,874] Trial 97 finished with value: 0.9057155661071467 and parameters: {'max_depth': 4, 'learning_rate': 0.21244163142173217}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:53:38,675] Trial 98 finished with value: 0.8838032329432165 and parameters: {'max_depth': 4, 'learning_rate': 0.0016762086112874718}. Best is trial 22 with value: 0.9067477482841423.\n",
            "[I 2025-04-15 21:53:44,286] Trial 99 finished with value: 0.9046538280011092 and parameters: {'max_depth': 2, 'learning_rate': 0.2658562287183016}. Best is trial 22 with value: 0.9067477482841423.\n"
          ]
        }
      ],
      "source": [
        "gb_study = run_study(gb_objective, \"GradientBoosting\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best model: LightGBM\n",
            "Best parameters: {'max_depth': 11, 'learning_rate': 0.06733966863133908}\n",
            "Best CV accuracy: 0.9086\n"
          ]
        }
      ],
      "source": [
        "# Find best model by accuracy\n",
        "studies = {\n",
        "    \"RandomForest\": rf_study,\n",
        "    \"XGBoost\": xgb_study,\n",
        "    \"LightGBM\": lgbm_study,\n",
        "    \"GradientBoosting\": gb_study\n",
        "}\n",
        "\n",
        "best_model_name = max(studies, key=lambda name: studies[name].best_value)\n",
        "best_study = studies[best_model_name]\n",
        "best_params = best_study.best_params\n",
        "best_cv_accuracy = best_study.best_value\n",
        "print(f\"Best model: {best_model_name}\")\n",
        "print(f\"Best parameters: {best_params}\")\n",
        "print(f\"Best CV accuracy: {best_cv_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 3940, number of negative: 29968\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005490 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 988\n",
            "[LightGBM] [Info] Number of data points in the train set: 33908, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116197 -> initscore=-2.028949\n",
            "[LightGBM] [Info] Start training from score -2.028949\n",
            "[LightGBM] [Info] Number of positive: 3940, number of negative: 29968\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001314 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 988\n",
            "[LightGBM] [Info] Number of data points in the train set: 33908, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116197 -> initscore=-2.028949\n",
            "[LightGBM] [Info] Start training from score -2.028949\n",
            "Best model: LightGBM\n",
            "Best hyperparameters: {'max_depth': 11, 'learning_rate': 0.06733966863133908, 'n_estimators': 100, 'random_state': 42}\n",
            "Cross-validation accuracy: 0.9086\n",
            "Test accuracy (tuned): 0.9049\n",
            "Test accuracy (default): 0.9074\n"
          ]
        }
      ],
      "source": [
        "#b\n",
        "# Define model with best hyperparameters\n",
        "def get_model(name, params):\n",
        "    params.update({\"n_estimators\": 100, \"random_state\": 42})\n",
        "    if name == \"RandomForest\":\n",
        "        return RandomForestClassifier(**params)\n",
        "    elif name == \"XGBoost\":\n",
        "        params.update({\"use_label_encoder\": False, \"eval_metric\": \"logloss\"})\n",
        "        return XGBClassifier(**params)\n",
        "    elif name == \"LightGBM\":\n",
        "        return LGBMClassifier(**params)\n",
        "    elif name == \"GradientBoosting\":\n",
        "        return GradientBoostingClassifier(**params)\n",
        "\n",
        "# Best tuned model\n",
        "best_model = get_model(best_model_name, best_params)\n",
        "best_model.fit(X_train, y_train)\n",
        "test_accuracy_tuned = accuracy_score(y_test, best_model.predict(X_test))\n",
        "\n",
        "# Compare with default model\n",
        "default_model = get_model(best_model_name, {})\n",
        "default_model.fit(X_train, y_train)\n",
        "test_accuracy_default = accuracy_score(y_test, default_model.predict(X_test))\n",
        "\n",
        "print(f\"Best model: {best_model_name}\")\n",
        "print(f\"Best hyperparameters: {best_params}\")\n",
        "print(f\"Cross-validation accuracy: {best_cv_accuracy:.4f}\")\n",
        "print(f\"Test accuracy (tuned): {test_accuracy_tuned:.4f}\")\n",
        "print(f\"Test accuracy (default): {test_accuracy_default:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR2k2_rfg5PD"
      },
      "source": [
        "For Q3 and Q4 ,use the following data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "1XezD-d0fKRD",
        "outputId": "b91f96ff-5d7b-4d41-84f2-eb41b73c1af7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Carat Weight</th>\n",
              "      <th>Cut</th>\n",
              "      <th>Color</th>\n",
              "      <th>Clarity</th>\n",
              "      <th>Polish</th>\n",
              "      <th>Symmetry</th>\n",
              "      <th>Report</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.10</td>\n",
              "      <td>Ideal</td>\n",
              "      <td>H</td>\n",
              "      <td>SI1</td>\n",
              "      <td>VG</td>\n",
              "      <td>EX</td>\n",
              "      <td>GIA</td>\n",
              "      <td>5169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.83</td>\n",
              "      <td>Ideal</td>\n",
              "      <td>H</td>\n",
              "      <td>VS1</td>\n",
              "      <td>ID</td>\n",
              "      <td>ID</td>\n",
              "      <td>AGSL</td>\n",
              "      <td>3470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.85</td>\n",
              "      <td>Ideal</td>\n",
              "      <td>H</td>\n",
              "      <td>SI1</td>\n",
              "      <td>EX</td>\n",
              "      <td>EX</td>\n",
              "      <td>GIA</td>\n",
              "      <td>3183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.91</td>\n",
              "      <td>Ideal</td>\n",
              "      <td>E</td>\n",
              "      <td>SI1</td>\n",
              "      <td>VG</td>\n",
              "      <td>VG</td>\n",
              "      <td>GIA</td>\n",
              "      <td>4370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.83</td>\n",
              "      <td>Ideal</td>\n",
              "      <td>G</td>\n",
              "      <td>SI1</td>\n",
              "      <td>EX</td>\n",
              "      <td>EX</td>\n",
              "      <td>GIA</td>\n",
              "      <td>3171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>1.03</td>\n",
              "      <td>Ideal</td>\n",
              "      <td>D</td>\n",
              "      <td>SI1</td>\n",
              "      <td>EX</td>\n",
              "      <td>EX</td>\n",
              "      <td>GIA</td>\n",
              "      <td>6250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>1.00</td>\n",
              "      <td>Very Good</td>\n",
              "      <td>D</td>\n",
              "      <td>SI1</td>\n",
              "      <td>VG</td>\n",
              "      <td>VG</td>\n",
              "      <td>GIA</td>\n",
              "      <td>5328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>1.02</td>\n",
              "      <td>Ideal</td>\n",
              "      <td>D</td>\n",
              "      <td>SI1</td>\n",
              "      <td>EX</td>\n",
              "      <td>EX</td>\n",
              "      <td>GIA</td>\n",
              "      <td>6157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>1.27</td>\n",
              "      <td>Signature-Ideal</td>\n",
              "      <td>G</td>\n",
              "      <td>VS1</td>\n",
              "      <td>EX</td>\n",
              "      <td>EX</td>\n",
              "      <td>GIA</td>\n",
              "      <td>11206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>2.19</td>\n",
              "      <td>Ideal</td>\n",
              "      <td>E</td>\n",
              "      <td>VS1</td>\n",
              "      <td>EX</td>\n",
              "      <td>EX</td>\n",
              "      <td>GIA</td>\n",
              "      <td>30507</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Carat Weight              Cut Color Clarity Polish Symmetry Report  \\\n",
              "0             1.10            Ideal     H     SI1     VG       EX    GIA   \n",
              "1             0.83            Ideal     H     VS1     ID       ID   AGSL   \n",
              "2             0.85            Ideal     H     SI1     EX       EX    GIA   \n",
              "3             0.91            Ideal     E     SI1     VG       VG    GIA   \n",
              "4             0.83            Ideal     G     SI1     EX       EX    GIA   \n",
              "...            ...              ...   ...     ...    ...      ...    ...   \n",
              "5995          1.03            Ideal     D     SI1     EX       EX    GIA   \n",
              "5996          1.00        Very Good     D     SI1     VG       VG    GIA   \n",
              "5997          1.02            Ideal     D     SI1     EX       EX    GIA   \n",
              "5998          1.27  Signature-Ideal     G     VS1     EX       EX    GIA   \n",
              "5999          2.19            Ideal     E     VS1     EX       EX    GIA   \n",
              "\n",
              "      Price  \n",
              "0      5169  \n",
              "1      3470  \n",
              "2      3183  \n",
              "3      4370  \n",
              "4      3171  \n",
              "...     ...  \n",
              "5995   6250  \n",
              "5996   5328  \n",
              "5997   6157  \n",
              "5998  11206  \n",
              "5999  30507  \n",
              "\n",
              "[6000 rows x 8 columns]"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dr=pd.read_csv('https://raw.githubusercontent.com/ogut77/DataScience/main/data/diamond.csv')\n",
        "dr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "zTuFQHpVfoxH"
      },
      "outputs": [],
      "source": [
        "def Encoder(df):\n",
        "          from sklearn import preprocessing\n",
        "          columnsToEncode = list(df.select_dtypes(include=['category','object']))\n",
        "          le = preprocessing.LabelEncoder()\n",
        "          for feature in columnsToEncode:\n",
        "              try:\n",
        "                  df[feature] = le.fit_transform(df[feature])\n",
        "              except:\n",
        "                  print('Error encoding '+feature)\n",
        "          return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "nxX6G5tNgWOh",
        "outputId": "e43a4e82-2a29-41b2-ed77-8e4c08994b03"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Carat Weight</th>\n",
              "      <th>Cut</th>\n",
              "      <th>Color</th>\n",
              "      <th>Clarity</th>\n",
              "      <th>Polish</th>\n",
              "      <th>Symmetry</th>\n",
              "      <th>Report</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.10</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.83</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.85</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.91</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.83</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>1.03</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>1.00</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>1.02</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>1.27</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>11206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>2.19</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>30507</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Carat Weight  Cut  Color  Clarity  Polish  Symmetry  Report  Price\n",
              "0             1.10    2      4        2       3         0       1   5169\n",
              "1             0.83    2      4        3       2         2       0   3470\n",
              "2             0.85    2      4        2       0         0       1   3183\n",
              "3             0.91    2      1        2       3         3       1   4370\n",
              "4             0.83    2      3        2       0         0       1   3171\n",
              "...            ...  ...    ...      ...     ...       ...     ...    ...\n",
              "5995          1.03    2      0        2       0         0       1   6250\n",
              "5996          1.00    4      0        2       3         3       1   5328\n",
              "5997          1.02    2      0        2       0         0       1   6157\n",
              "5998          1.27    3      3        3       0         0       1  11206\n",
              "5999          2.19    2      1        3       0         0       1  30507\n",
              "\n",
              "[6000 rows x 8 columns]"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dr=Encoder(dr)\n",
        "dr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Kv7SnSAahe1-",
        "outputId": "1ab53992-c642-474d-bcfa-ad617fb1b660"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Carat Weight</th>\n",
              "      <th>Cut</th>\n",
              "      <th>Color</th>\n",
              "      <th>Clarity</th>\n",
              "      <th>Polish</th>\n",
              "      <th>Symmetry</th>\n",
              "      <th>Report</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.10</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.83</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.85</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.91</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.83</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>1.03</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>1.00</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>1.02</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>1.27</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>2.19</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Carat Weight  Cut  Color  Clarity  Polish  Symmetry  Report\n",
              "0             1.10    2      4        2       3         0       1\n",
              "1             0.83    2      4        3       2         2       0\n",
              "2             0.85    2      4        2       0         0       1\n",
              "3             0.91    2      1        2       3         3       1\n",
              "4             0.83    2      3        2       0         0       1\n",
              "...            ...  ...    ...      ...     ...       ...     ...\n",
              "5995          1.03    2      0        2       0         0       1\n",
              "5996          1.00    4      0        2       3         3       1\n",
              "5997          1.02    2      0        2       0         0       1\n",
              "5998          1.27    3      3        3       0         0       1\n",
              "5999          2.19    2      1        3       0         0       1\n",
              "\n",
              "[6000 rows x 7 columns]"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = dr['Price'] #Output\n",
        "X = dr.drop('Price',axis=1)\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "KOLT3hfbgz7F"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import  train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=17)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "import optuna\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzkaMEVGZ0pS"
      },
      "source": [
        "Q3)Using Linear Regression,Decison Tree Random Forest,XGBoost, Light GBM and Gradient Boosting Classifier with default parameters (no parameter specifications except random_state) calculate R2 statistics on test data. Which method gives the best accuracy on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LinearRegression R²: 0.8234\n",
            "DecisionTree R²: 0.9588\n",
            "RandomForest R²: 0.9807\n",
            "XGBoost R²: 0.9809\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000040 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 193\n",
            "[LightGBM] [Info] Number of data points in the train set: 4500, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 11827.946667\n",
            "LightGBM R²: 0.9813\n",
            "GradientBoosting R²: 0.9739\n",
            "\n",
            "Best model based on default parameters: LightGBM with R² = 0.9813\n"
          ]
        }
      ],
      "source": [
        "models = {\n",
        "    \"LinearRegression\": LinearRegression(),\n",
        "    \"DecisionTree\": DecisionTreeRegressor(random_state=42),\n",
        "    \"RandomForest\": RandomForestRegressor(random_state=42),\n",
        "    \"XGBoost\": XGBRegressor(random_state=42),\n",
        "    \"LightGBM\": LGBMRegressor(random_state=42),\n",
        "    \"GradientBoosting\": GradientBoostingRegressor(random_state=42)\n",
        "}\n",
        "\n",
        "r2_scores = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test)\n",
        "    r2 = r2_score(y_test, preds)\n",
        "    r2_scores[name] = r2\n",
        "    print(f\"{name} R²: {r2:.4f}\")\n",
        "\n",
        "# Find best model\n",
        "best_model_name = max(r2_scores, key=r2_scores.get)\n",
        "print(f\"\\nBest model based on default parameters: {best_model_name} with R² = {r2_scores[best_model_name]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nksEpGl5bDv3"
      },
      "source": [
        "Q4) Using optuna hyperparmeter optimization technique (100 trial)  with Random Forest,XGBoost, Light GBM and Gradient Boosting Regressor\n",
        "\n",
        "\n",
        "a)find best methods with  parameters  using Cross validation (CV=3) technique for the range of   parameters below. What are the best parameters for the method with highest cross validation R2?\n",
        "\n",
        "For random forest\n",
        "\n",
        "\n",
        "  \"max_depth\"   : trial.suggest_int(\"max_depth\", 2,  X_train.shape[1]),\n",
        "  \"max_features\": trial.suggest_int(\"max_features\", 2, X_train.shape[1])\n",
        "\n",
        "For XGBoost, Light GBM and Gradient Boosting Classifier\n",
        "\n",
        "  \"max_depth\": trial.suggest_int(\"max_depth\", 2, X_train.shape[1]),\n",
        "  \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.3,log=True)\n",
        "\n",
        "where X_train.shape[1] is number of columnns in the train data.\n",
        "\n",
        " b)Evaluate the performance of the  method with highest cross validation R2 on test data. What is the R2 value? Are there any improvement of the same method with default parameters?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-15 21:53:47,013] A new study created in memory with name: RandomForestRegressor\n",
            "[I 2025-04-15 21:53:47,331] Trial 0 finished with value: 0.7948719878863658 and parameters: {'max_depth': 4, 'max_features': 2}. Best is trial 0 with value: 0.7948719878863658.\n",
            "[I 2025-04-15 21:53:47,602] Trial 1 finished with value: 0.5632525526916333 and parameters: {'max_depth': 2, 'max_features': 2}. Best is trial 0 with value: 0.7948719878863658.\n",
            "[I 2025-04-15 21:53:47,870] Trial 2 finished with value: 0.5632525526916333 and parameters: {'max_depth': 2, 'max_features': 2}. Best is trial 0 with value: 0.7948719878863658.\n",
            "[I 2025-04-15 21:53:48,531] Trial 3 finished with value: 0.9610298378466339 and parameters: {'max_depth': 6, 'max_features': 7}. Best is trial 3 with value: 0.9610298378466339.\n",
            "[I 2025-04-15 21:53:49,178] Trial 4 finished with value: 0.9718580954876749 and parameters: {'max_depth': 7, 'max_features': 6}. Best is trial 4 with value: 0.9718580954876749.\n",
            "[I 2025-04-15 21:53:49,498] Trial 5 finished with value: 0.7930413906109887 and parameters: {'max_depth': 3, 'max_features': 3}. Best is trial 4 with value: 0.9718580954876749.\n",
            "[I 2025-04-15 21:53:49,925] Trial 6 finished with value: 0.9175511425550704 and parameters: {'max_depth': 4, 'max_features': 5}. Best is trial 4 with value: 0.9718580954876749.\n",
            "[I 2025-04-15 21:53:50,512] Trial 7 finished with value: 0.9632092426756178 and parameters: {'max_depth': 6, 'max_features': 6}. Best is trial 4 with value: 0.9718580954876749.\n",
            "[I 2025-04-15 21:53:50,980] Trial 8 finished with value: 0.9175602303021698 and parameters: {'max_depth': 4, 'max_features': 6}. Best is trial 4 with value: 0.9718580954876749.\n",
            "[I 2025-04-15 21:53:51,262] Trial 9 finished with value: 0.7930413906109887 and parameters: {'max_depth': 3, 'max_features': 3}. Best is trial 4 with value: 0.9718580954876749.\n",
            "[I 2025-04-15 21:53:52,009] Trial 10 finished with value: 0.9710286162519403 and parameters: {'max_depth': 7, 'max_features': 7}. Best is trial 4 with value: 0.9718580954876749.\n",
            "[I 2025-04-15 21:53:52,760] Trial 11 finished with value: 0.9710286162519403 and parameters: {'max_depth': 7, 'max_features': 7}. Best is trial 4 with value: 0.9718580954876749.\n",
            "[I 2025-04-15 21:53:53,400] Trial 12 finished with value: 0.9718580954876749 and parameters: {'max_depth': 7, 'max_features': 6}. Best is trial 4 with value: 0.9718580954876749.\n",
            "[I 2025-04-15 21:53:53,921] Trial 13 finished with value: 0.9621371200856855 and parameters: {'max_depth': 6, 'max_features': 5}. Best is trial 4 with value: 0.9718580954876749.\n",
            "[I 2025-04-15 21:53:54,583] Trial 14 finished with value: 0.9718580954876749 and parameters: {'max_depth': 7, 'max_features': 6}. Best is trial 4 with value: 0.9718580954876749.\n",
            "[I 2025-04-15 21:53:55,026] Trial 15 finished with value: 0.9412380983664911 and parameters: {'max_depth': 5, 'max_features': 4}. Best is trial 4 with value: 0.9718580954876749.\n",
            "[I 2025-04-15 21:53:55,527] Trial 16 finished with value: 0.9456303419347601 and parameters: {'max_depth': 5, 'max_features': 5}. Best is trial 4 with value: 0.9718580954876749.\n",
            "[I 2025-04-15 21:53:56,119] Trial 17 finished with value: 0.9632092426756178 and parameters: {'max_depth': 6, 'max_features': 6}. Best is trial 4 with value: 0.9718580954876749.\n",
            "[I 2025-04-15 21:53:56,647] Trial 18 finished with value: 0.9685142542551749 and parameters: {'max_depth': 7, 'max_features': 4}. Best is trial 4 with value: 0.9718580954876749.\n",
            "[I 2025-04-15 21:53:57,276] Trial 19 finished with value: 0.9436686356221937 and parameters: {'max_depth': 5, 'max_features': 6}. Best is trial 4 with value: 0.9718580954876749.\n",
            "[I 2025-04-15 21:53:58,156] Trial 20 finished with value: 0.9710286162519403 and parameters: {'max_depth': 7, 'max_features': 7}. Best is trial 4 with value: 0.9718580954876749.\n",
            "[I 2025-04-15 21:53:59,052] Trial 21 finished with value: 0.9718580954876749 and parameters: {'max_depth': 7, 'max_features': 6}. Best is trial 4 with value: 0.9718580954876749.\n",
            "[I 2025-04-15 21:53:59,759] Trial 22 finished with value: 0.9621371200856855 and parameters: {'max_depth': 6, 'max_features': 5}. Best is trial 4 with value: 0.9718580954876749.\n",
            "[I 2025-04-15 21:54:00,651] Trial 23 finished with value: 0.9718580954876749 and parameters: {'max_depth': 7, 'max_features': 6}. Best is trial 4 with value: 0.9718580954876749.\n",
            "[I 2025-04-15 21:54:01,344] Trial 24 finished with value: 0.9621371200856855 and parameters: {'max_depth': 6, 'max_features': 5}. Best is trial 4 with value: 0.9718580954876749.\n",
            "[I 2025-04-15 21:54:02,215] Trial 25 finished with value: 0.9710286162519403 and parameters: {'max_depth': 7, 'max_features': 7}. Best is trial 4 with value: 0.9718580954876749.\n",
            "[I 2025-04-15 21:54:02,988] Trial 26 finished with value: 0.9632092426756178 and parameters: {'max_depth': 6, 'max_features': 6}. Best is trial 4 with value: 0.9718580954876749.\n",
            "[I 2025-04-15 21:54:03,635] Trial 27 finished with value: 0.9685142542551749 and parameters: {'max_depth': 7, 'max_features': 4}. Best is trial 4 with value: 0.9718580954876749.\n",
            "[I 2025-04-15 21:54:04,233] Trial 28 finished with value: 0.9436686356221937 and parameters: {'max_depth': 5, 'max_features': 6}. Best is trial 4 with value: 0.9718580954876749.\n",
            "[I 2025-04-15 21:54:04,963] Trial 29 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:05,609] Trial 30 finished with value: 0.9621371200856855 and parameters: {'max_depth': 6, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:06,428] Trial 31 finished with value: 0.9718580954876749 and parameters: {'max_depth': 7, 'max_features': 6}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:07,197] Trial 32 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:07,830] Trial 33 finished with value: 0.9685142542551749 and parameters: {'max_depth': 7, 'max_features': 4}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:08,536] Trial 34 finished with value: 0.9621371200856855 and parameters: {'max_depth': 6, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:08,996] Trial 35 finished with value: 0.8619802742722952 and parameters: {'max_depth': 3, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:09,546] Trial 36 finished with value: 0.9531477012402831 and parameters: {'max_depth': 7, 'max_features': 3}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:10,161] Trial 37 finished with value: 0.9581176035653959 and parameters: {'max_depth': 6, 'max_features': 4}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:10,645] Trial 38 finished with value: 0.7692211216798593 and parameters: {'max_depth': 2, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:11,180] Trial 39 finished with value: 0.9096929349467642 and parameters: {'max_depth': 7, 'max_features': 2}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:11,943] Trial 40 finished with value: 0.9610298378466339 and parameters: {'max_depth': 6, 'max_features': 7}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:12,802] Trial 41 finished with value: 0.9718580954876749 and parameters: {'max_depth': 7, 'max_features': 6}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:13,540] Trial 42 finished with value: 0.9718580954876749 and parameters: {'max_depth': 7, 'max_features': 6}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:14,275] Trial 43 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:15,003] Trial 44 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:15,607] Trial 45 finished with value: 0.9621371200856855 and parameters: {'max_depth': 6, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:16,351] Trial 46 finished with value: 0.9685142542551749 and parameters: {'max_depth': 7, 'max_features': 4}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:17,068] Trial 47 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:17,771] Trial 48 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:18,368] Trial 49 finished with value: 0.9621371200856855 and parameters: {'max_depth': 6, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:18,837] Trial 50 finished with value: 0.9070442772007424 and parameters: {'max_depth': 4, 'max_features': 4}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:19,486] Trial 51 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:20,236] Trial 52 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:21,138] Trial 53 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:21,905] Trial 54 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:22,631] Trial 55 finished with value: 0.9581176035653959 and parameters: {'max_depth': 6, 'max_features': 4}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:23,635] Trial 56 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:24,420] Trial 57 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:25,212] Trial 58 finished with value: 0.9685142542551749 and parameters: {'max_depth': 7, 'max_features': 4}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:26,038] Trial 59 finished with value: 0.9456303419347601 and parameters: {'max_depth': 5, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:26,829] Trial 60 finished with value: 0.9621371200856855 and parameters: {'max_depth': 6, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:27,754] Trial 61 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:28,637] Trial 62 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:29,418] Trial 63 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:30,308] Trial 64 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:30,897] Trial 65 finished with value: 0.746465713637885 and parameters: {'max_depth': 2, 'max_features': 4}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:31,802] Trial 66 finished with value: 0.9718580954876749 and parameters: {'max_depth': 7, 'max_features': 6}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:32,519] Trial 67 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:33,493] Trial 68 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:34,321] Trial 69 finished with value: 0.9632092426756178 and parameters: {'max_depth': 6, 'max_features': 6}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:34,837] Trial 70 finished with value: 0.8453652840380785 and parameters: {'max_depth': 3, 'max_features': 4}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:35,584] Trial 71 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:36,380] Trial 72 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:37,148] Trial 73 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:37,904] Trial 74 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:38,880] Trial 75 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:39,826] Trial 76 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:40,594] Trial 77 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:41,426] Trial 78 finished with value: 0.9632092426756178 and parameters: {'max_depth': 6, 'max_features': 6}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:42,103] Trial 79 finished with value: 0.9685142542551749 and parameters: {'max_depth': 7, 'max_features': 4}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:42,765] Trial 80 finished with value: 0.9621371200856855 and parameters: {'max_depth': 6, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:43,590] Trial 81 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:44,318] Trial 82 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:45,076] Trial 83 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:46,065] Trial 84 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:46,833] Trial 85 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:47,761] Trial 86 finished with value: 0.9718580954876749 and parameters: {'max_depth': 7, 'max_features': 6}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:48,515] Trial 87 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:49,283] Trial 88 finished with value: 0.9685142542551749 and parameters: {'max_depth': 7, 'max_features': 4}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:50,212] Trial 89 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:51,005] Trial 90 finished with value: 0.9632092426756178 and parameters: {'max_depth': 6, 'max_features': 6}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:51,821] Trial 91 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:52,753] Trial 92 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:53,589] Trial 93 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:54,509] Trial 94 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:55,277] Trial 95 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:56,071] Trial 96 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:57,047] Trial 97 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:57,921] Trial 98 finished with value: 0.9718893251991633 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 29 with value: 0.9718893251991633.\n",
            "[I 2025-04-15 21:54:58,410] Trial 99 finished with value: 0.7948719878863658 and parameters: {'max_depth': 4, 'max_features': 2}. Best is trial 29 with value: 0.9718893251991633.\n"
          ]
        }
      ],
      "source": [
        "#a\n",
        "n_features = X_train.shape[1]\n",
        "\n",
        "def rf_objective(trial):\n",
        "    params = {\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, n_features),\n",
        "        \"max_features\": trial.suggest_int(\"max_features\", 2, n_features),\n",
        "        \"n_estimators\": 100,\n",
        "        \"random_state\": 42\n",
        "    }\n",
        "    model = RandomForestRegressor(**params)\n",
        "    return cross_val_score(model, X_train, y_train, cv=3, scoring=\"r2\").mean()\n",
        "\n",
        "def xgb_objective(trial):\n",
        "    params = {\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, n_features),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.3, log=True),\n",
        "        \"n_estimators\": 100,\n",
        "        \"random_state\": 42\n",
        "    }\n",
        "    model = XGBRegressor(**params)\n",
        "    return cross_val_score(model, X_train, y_train, cv=3, scoring=\"r2\").mean()\n",
        "\n",
        "def lgbm_objective(trial):\n",
        "    params = {\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, n_features),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.3, log=True),\n",
        "        \"n_estimators\": 100,\n",
        "        \"random_state\": 42\n",
        "    }\n",
        "    model = LGBMRegressor(**params)\n",
        "    return cross_val_score(model, X_train, y_train, cv=3, scoring=\"r2\").mean()\n",
        "\n",
        "def gb_objective(trial):\n",
        "    params = {\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, n_features),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.3, log=True),\n",
        "        \"n_estimators\": 100,\n",
        "        \"random_state\": 42\n",
        "    }\n",
        "    model = GradientBoostingRegressor(**params)\n",
        "    return cross_val_score(model, X_train, y_train, cv=3, scoring=\"r2\").mean()\n",
        "\n",
        "# Run Optuna studies\n",
        "def run_study(objective_func, name):\n",
        "    study = optuna.create_study(direction=\"maximize\", study_name=name)\n",
        "    study.optimize(objective_func, n_trials=100)\n",
        "    return study\n",
        "\n",
        "rf_study = run_study(rf_objective, \"RandomForestRegressor\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgb_study = run_study(xgb_objective, \"XGBRegressor\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lgbm_study = run_study(lgbm_objective, \"LGBMRegressor\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gb_study = run_study(gb_objective, \"GradientBoostingRegressor\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare best R² scores\n",
        "studies = {\n",
        "    \"RandomForest\": rf_study,\n",
        "    \"XGBoost\": xgb_study,\n",
        "    \"LightGBM\": lgbm_study,\n",
        "    \"GradientBoosting\": gb_study\n",
        "}\n",
        "\n",
        "best_model_name = max(studies, key=lambda name: studies[name].best_value)\n",
        "best_study = studies[best_model_name]\n",
        "best_params = best_study.best_params\n",
        "best_cv_r2 = best_study.best_value\n",
        "\n",
        "print(f\"\\nBest Model: {best_model_name}\")\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "print(f\"Best Cross-Validation R²: {best_cv_r2:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000071 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 193\n",
            "[LightGBM] [Info] Number of data points in the train set: 4500, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 11827.946667\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000040 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 193\n",
            "[LightGBM] [Info] Number of data points in the train set: 4500, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 11827.946667\n",
            "Test R² (tuned): 0.9822\n",
            "Test R² (default): 0.9813\n"
          ]
        }
      ],
      "source": [
        "#b\n",
        "def get_regressor(name, params):\n",
        "    params.update({\"n_estimators\": 100, \"random_state\": 42})\n",
        "    if name == \"RandomForest\":\n",
        "        return RandomForestRegressor(**params)\n",
        "    elif name == \"XGBoost\":\n",
        "        return XGBRegressor(**params)\n",
        "    elif name == \"LightGBM\":\n",
        "        return LGBMRegressor(**params)\n",
        "    elif name == \"GradientBoosting\":\n",
        "        return GradientBoostingRegressor(**params)\n",
        "\n",
        "# Best tuned model\n",
        "best_model = get_regressor(best_model_name, best_params)\n",
        "best_model.fit(X_train, y_train)\n",
        "test_r2_tuned = r2_score(y_test, best_model.predict(X_test))\n",
        "\n",
        "# Compare with default version of same model\n",
        "default_model = get_regressor(best_model_name, {})\n",
        "default_model.fit(X_train, y_train)\n",
        "test_r2_default = r2_score(y_test, default_model.predict(X_test))\n",
        "\n",
        "print(f\"Test R² (tuned): {test_r2_tuned:.4f}\")\n",
        "print(f\"Test R² (default): {test_r2_default:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "Homework5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
