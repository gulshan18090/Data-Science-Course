{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "o0M6wvVafclb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "url = \"https://raw.githubusercontent.com/ogut77/DataScience/master/insurance.csv\"\n",
        "df = pd.read_csv(url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "CbamjUmFY-tK",
        "outputId": "1f31a20b-2b52-49ed-fdf9-37da0529a8c8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>bmi</th>\n",
              "      <th>children</th>\n",
              "      <th>smoker</th>\n",
              "      <th>region</th>\n",
              "      <th>charges</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>19</td>\n",
              "      <td>female</td>\n",
              "      <td>27.900</td>\n",
              "      <td>0</td>\n",
              "      <td>yes</td>\n",
              "      <td>southwest</td>\n",
              "      <td>16884.92400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18</td>\n",
              "      <td>male</td>\n",
              "      <td>33.770</td>\n",
              "      <td>1</td>\n",
              "      <td>no</td>\n",
              "      <td>southeast</td>\n",
              "      <td>1725.55230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>28</td>\n",
              "      <td>male</td>\n",
              "      <td>33.000</td>\n",
              "      <td>3</td>\n",
              "      <td>no</td>\n",
              "      <td>southeast</td>\n",
              "      <td>4449.46200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33</td>\n",
              "      <td>male</td>\n",
              "      <td>22.705</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>northwest</td>\n",
              "      <td>21984.47061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>32</td>\n",
              "      <td>male</td>\n",
              "      <td>28.880</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>northwest</td>\n",
              "      <td>3866.85520</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age     sex     bmi  children smoker     region      charges\n",
              "0   19  female  27.900         0    yes  southwest  16884.92400\n",
              "1   18    male  33.770         1     no  southeast   1725.55230\n",
              "2   28    male  33.000         3     no  southeast   4449.46200\n",
              "3   33    male  22.705         0     no  northwest  21984.47061\n",
              "4   32    male  28.880         0     no  northwest   3866.85520"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYwzA6LQjQhR"
      },
      "source": [
        "Context in Insurance Data\n",
        "This dataset is often used to predict charges based on the other variables (age, sex, bmi, children, smoker, region). For example:\n",
        "\n",
        "Input Variables (X): age, sex, bmi, children, smoker, region (features used to make predictions).\n",
        "\n",
        "Output Variable (y): charges (what you’re trying to predict)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fya2Ad-i8yt"
      },
      "source": [
        "Describtion of variables\n",
        "1. Age\n",
        "Description: The age of the individual (the insured person).\n",
        "Type: Numerical (integer).\n",
        "Example Values: 19, 45, 62, etc.\n",
        "Role in Insurance: Age is a key factor in determining insurance charges. Older individuals often have higher medical costs (and thus higher charges) due to increased health risks.\n",
        "2. Sex\n",
        "Description: The gender of the individual.\n",
        "Type: Categorical (text or binary).\n",
        "Example Values: \"male,\" \"female\"\n",
        "Role in Insurance: Gender can influence insurance charges because health risks and medical expenses may differ between males and females (e.g., pregnancy-related costs for females).\n",
        "3. BMI (Body Mass Index)\n",
        "Description: A measure of body fat based on height and weight (calculated as weight in kg divided by height in meters squared).\n",
        "Type: Numerical (float).\n",
        "Example Values: 25.3, 30.1, 18.5, etc.\n",
        "Role in Insurance: Higher BMI often correlates with increased health risks (e.g., obesity-related conditions like diabetes or heart disease), leading to higher insurance charges.\n",
        "4. Children\n",
        "Description: The number of children (dependents) covered under the individual’s insurance plan.\n",
        "Type: Numerical (integer).\n",
        "Example Values: 0, 1, 3, etc.\n",
        "Role in Insurance: More children can increase insurance costs slightly, as it may reflect additional healthcare needs, though the effect is often less pronounced than other factors like smoking or age.\n",
        "5. Smoker\n",
        "Description: Indicates whether the individual smokes tobacco.\n",
        "Type: Categorical (text or binary).\n",
        "Example Values: \"yes,\" \"no\" .\n",
        "Role in Insurance: Smoking is a major factor in insurance charges. Smokers typically have much higher medical costs due to risks like lung disease or cancer, so their charges are significantly elevated.\n",
        "6. Region\n",
        "Description: The geographic region where the individual lives.\n",
        "Type: Categorical (text).\n",
        "Example Values: \"northeast,\" \"southeast,\" \"southwest,\" \"northwest\" (common in U.S.-based datasets).\n",
        "Role in Insurance: Charges can vary by region due to differences in healthcare costs, lifestyle factors, or local insurance regulations.\n",
        "7. Charges\n",
        "Description: The insurance charges (or premiums/costs) billed to the individual, typically in a currency like USD.\n",
        "Type: Numerical (float).\n",
        "Example Values: 1684.52, 11234.89, 32050.23, etc.\n",
        "Role in Insurance: This is usually the target variable (output) in predictive modeling. It represents the amount the insurance company charges, influenced by all the other columns (age, sex, BMI, etc.).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor    \n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "KkwJgK18ZQMY"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "age         0\n",
              "sex         0\n",
              "bmi         0\n",
              "children    0\n",
              "smoker      0\n",
              "region      0\n",
              "charges     0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#1. Check if there is null value in dataset df (5 pt)\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7XfDiXfTZnpk"
      },
      "outputs": [],
      "source": [
        "#2. Assign charges to y  and others to X using df. y is output variable and X is input variables (5 pt)\n",
        "y=df['charges']\n",
        "X=df.drop('charges',axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "e5djdXCnhxpy"
      },
      "outputs": [],
      "source": [
        "#3. Use  get_dummies() function from the pandas library to convert categorical variables in a DataFrame (X).\n",
        "# Drop first drops the first category’s dummy variable to avoid multicollinearity (5 pt)\n",
        "X=pd.get_dummies(X, drop_first=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "UoTK1j0psi0-"
      },
      "outputs": [],
      "source": [
        "#Use following methods for the evaluation on test and train data\n",
        "def evalmetric(y,ypred):\n",
        " from scipy.stats import pearsonr\n",
        " import numpy as np\n",
        " e = y - ypred\n",
        " mse_f = np.mean(e**2)\n",
        " rmse_f = np.sqrt(mse_f)\n",
        " mae_f = np.mean(abs(e))\n",
        " mape_f = 100*np.mean(abs(e/y))\n",
        " crl, _ = pearsonr(y, ypred)\n",
        " r2_f = crl*crl\n",
        " print(\"MSE:\", mse_f)\n",
        " print(\"RMSE:\", rmse_f)\n",
        " print(\"MAE:\",mae_f)\n",
        " print(\"MAPE:\",mape_f)\n",
        " print(\"R-Squared:\", round(r2_f, 4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "37xaAIN_nNqq"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "age                 0.299008\n",
              "bmi                 0.198341\n",
              "children            0.067998\n",
              "sex_male            0.057292\n",
              "smoker_yes          0.787251\n",
              "region_northwest   -0.039905\n",
              "region_southeast    0.073982\n",
              "region_southwest   -0.043210\n",
              "dtype: float64"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#4.Get the correlation between X variables and y variables.(5 pt)\n",
        "X.corrwith(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Tqgr4WFjnViT"
      },
      "outputs": [],
      "source": [
        "#5.Split a dataset into 25%  of data as test data  and 75% of data as training data ( pt)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gm6bNy8neAl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision Tree\n",
            "MSE: 37647892.74363372\n",
            "RMSE: 6135.787866577015\n",
            "MAE: 2706.370587474627\n",
            "MAPE: 30.610239392826255\n",
            "R-Squared: 0.7745\n",
            "Linear Regression\n",
            "MSE: 35117755.73613632\n",
            "RMSE: 5926.023602394469\n",
            "MAE: 4243.654116653135\n",
            "MAPE: 44.46818511698092\n",
            "R-Squared: 0.7676\n"
          ]
        }
      ],
      "source": [
        "#6. Using Decision Tree and Linear Regression methods, compare the performance results on both test and training data\n",
        "#to determine which one is more likely to overfit and which is more likely to underfit.\n",
        "# Do you think that Lasso and Ridge regularization are more likely to improve the results of Linear model test data,\n",
        "# or would Random Forest or Boosting methods are more likely to improve the results of Decison tree test data?\n",
        "#Explain your reasoning.(35 pt)\n",
        "\n",
        "#Decision Tree\n",
        "dt = DecisionTreeRegressor()\n",
        "dt.fit(x_train, y_train)\n",
        "y_pred = dt.predict(x_test)\n",
        "print(\"Decision Tree\")\n",
        "evalmetric(y_test, y_pred)\n",
        "\n",
        "#Linear Regression\n",
        "lr = LinearRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "y_pred = lr.predict(x_test)\n",
        "print(\"Linear Regression\")\n",
        "evalmetric(y_test, y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6.\n",
        "- **Decision Tree (R² = 0.7745)** captures variance well but tends to overfit.  \n",
        "- **Linear Regression (R² = 0.7676)** underfits due to its assumption of linearity, leading to higher errors.  \n",
        "\n",
        "### **Regularization vs. Ensemble Methods**  \n",
        "\n",
        "- **Lasso & Ridge** improve Linear Regression by preventing overfitting but may not solve underfitting.  \n",
        "- **Random Forest** reduces Decision Tree overfitting by averaging multiple trees.  \n",
        "- **Boosting (XGBoost, LightGBM, CatBoost)** corrects errors sequentially, enhancing accuracy.  \n",
        "\n",
        "### **Best Approach?**  \n",
        "- **Boosting (especially CatBoost, R² = 0.8582) outperforms Random Forest (R² = 0.8534)**.  \n",
        "- **Ensemble methods are better than regularization for improving Decision Tree performance**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ru5josYAR_gH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear Regression\n",
            "MSE: 35117755.73613632\n",
            "RMSE: 5926.023602394469\n",
            "MAE: 4243.654116653135\n",
            "MAPE: 44.46818511698092\n",
            "R-Squared: 0.7676\n"
          ]
        }
      ],
      "source": [
        "#7. Explain performance of linear regressin on test data\n",
        "# using  Root mean squared error, mean absolute error, mean absolute percentage error and R2 metric (10 pt)\n",
        "#Linear Regression\n",
        "print(\"Linear Regression\")\n",
        "evalmetric(y_test, y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "7.\n",
        "\n",
        "- **RMSE (5926.02)**: High error magnitude, some large deviations.  \n",
        "- **MAE (4243.65)**: Significant average prediction error.  \n",
        "- **MAPE (44.47%)**: Poor relative accuracy, indicating underfitting.  \n",
        "- **R² (0.7676)**: Explains 76.76% of variance but lacks full predictive power.  \n",
        "\n",
        "**Conclusion**: The model struggles with non-linearity, leading to underfitting. Regularization (Lasso/Ridge) or feature engineering could improve results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4g6E7l-hqPjK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest\n",
            "MSE: 22642593.191493455\n",
            "RMSE: 4758.423393466943\n",
            "MAE: 2653.3516677551743\n",
            "MAPE: 30.574901715988634\n",
            "R-Squared: 0.8534\n",
            "XGBoost\n",
            "MSE: 26433443.13176504\n",
            "RMSE: 5141.346431798293\n",
            "MAE: 2957.213261792119\n",
            "MAPE: 34.57266690344694\n",
            "R-Squared: 0.8301\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000096 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 1003, number of used features: 8\n",
            "[LightGBM] [Info] Start training from score 13267.935814\n",
            "LightGBM\n",
            "MSE: 22005117.949021608\n",
            "RMSE: 4690.961303296117\n",
            "MAE: 2700.720352413941\n",
            "MAPE: 33.101463593940274\n",
            "R-Squared: 0.8553\n",
            "CatBoost\n",
            "MSE: 21448141.827996742\n",
            "RMSE: 4631.213861181185\n",
            "MAE: 2607.3449326649175\n",
            "MAPE: 32.198823118532935\n",
            "R-Squared: 0.8582\n"
          ]
        }
      ],
      "source": [
        "#8. Use Random Forest and Boosting methods (XGBoost, LightGBM, and CatBoost)\n",
        "#to obtain the evaluation scores on  test data.\n",
        "#Which Boosting technique yielded the best performance on the test data based on the R² metric?\n",
        "#Did you achieve a better result compared to Random Forest on the test data based on the R² metric?\n",
        "#If there is improvement on Random forest or boosting methods over decison tree, explain  (30 pt)\n",
        "\n",
        "#Random Forest\n",
        "rf = RandomForestRegressor()\n",
        "rf.fit(x_train, y_train)\n",
        "y_pred = rf.predict(x_test)\n",
        "print(\"Random Forest\")\n",
        "evalmetric(y_test, y_pred)\n",
        "\n",
        "#XGBoost\n",
        "xgb = XGBRegressor()\n",
        "xgb.fit(x_train, y_train)\n",
        "y_pred = xgb.predict(x_test)\n",
        "print(\"XGBoost\")\n",
        "evalmetric(y_test, y_pred)\n",
        "\n",
        "#LightGBM\n",
        "lgb = LGBMRegressor()\n",
        "lgb.fit(x_train, y_train)\n",
        "y_pred = lgb.predict(x_test)\n",
        "print(\"LightGBM\")\n",
        "evalmetric(y_test, y_pred)\n",
        "\n",
        "#CatBoost\n",
        "\n",
        "cb = CatBoostRegressor(verbose=0)\n",
        "cb.fit(x_train, y_train)\n",
        "y_pred = cb.predict(x_test)\n",
        "print(\"CatBoost\")\n",
        "evalmetric(y_test, y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Best Model Based on R²**  \n",
        "- **CatBoost (R² = 0.8582) performed best**, followed by LightGBM (0.8553) and Random Forest (0.8534).  \n",
        "- **XGBoost had the lowest R² (0.8301) among the ensemble models.**  \n",
        "\n",
        "### **Improvement Over Decision Tree**  \n",
        "- **All ensemble models outperformed Decision Tree (R² = 0.7745), reducing overfitting.**  \n",
        "- **Boosting methods (especially CatBoost) improved accuracy by sequentially correcting errors.**  \n",
        "\n",
        "### **Conclusion**  \n",
        "- **CatBoost showed the best generalization, outperforming Random Forest and other boosting methods.**  \n",
        "- **Ensemble methods are more effective than Decision Trees alone due to better variance control.**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
